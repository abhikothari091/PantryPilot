{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üç≥ PantryPilot: Llama 3.2 3B Fine-tuning on Lambda Labs A100\n",
    "\n",
    "**Optimized for Lambda Labs A100 GPU**\n",
    "\n",
    "- Expected training time: **45-60 minutes**\n",
    "- Expected cost: **$0.83 - $1.10**\n",
    "- GPU: A100 40GB\n",
    "- Speed: ~0.5-0.8 it/s\n",
    "\n",
    "## üìã Prerequisites\n",
    "1. Lambda Labs account with credits\n",
    "2. A100 instance running\n",
    "3. Training data files:\n",
    "   - `recipes_train_chat.jsonl` (13 MB)\n",
    "   - `recipes_val_chat.jsonl` (1.7 MB)\n",
    "4. HuggingFace token with Llama 3.2 access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q transformers==4.57.1 peft==0.18.0 accelerate bitsandbytes datasets tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê HuggingFace Authentication\n",
    "\n",
    "Enter your HuggingFace token: `YOUR_HUGGINGFACE_TOKEN_HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Direct token login (for automation)\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"  # Replace with your token\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# Option 2: Interactive login (uncomment to use)\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "print(\"‚úÖ Logged in to HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Upload Training Data\n",
    "\n",
    "**Upload files using one of these methods:**\n",
    "\n",
    "### Method 1: JupyterLab File Upload\n",
    "1. Click the upload button in JupyterLab file browser\n",
    "2. Upload `recipes_train_chat.jsonl` and `recipes_val_chat.jsonl`\n",
    "3. Move them to `data/` folder\n",
    "\n",
    "### Method 2: SCP from local machine\n",
    "```bash\n",
    "# From your local terminal:\n",
    "scp recipes_train_chat.jsonl ubuntu@<lambda-ip>:~/data/\n",
    "scp recipes_val_chat.jsonl ubuntu@<lambda-ip>:~/data/\n",
    "```\n",
    "\n",
    "### Method 3: Python upload widget (below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "!mkdir -p data\n",
    "\n",
    "# Check if files exist\n",
    "import os\n",
    "train_file = \"data/recipes_train_chat.jsonl\"\n",
    "val_file = \"data/recipes_val_chat.jsonl\"\n",
    "\n",
    "if os.path.exists(train_file) and os.path.exists(val_file):\n",
    "    print(\"‚úÖ Training data found!\")\n",
    "    !ls -lh data/\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please upload training data files to the data/ directory\")\n",
    "    print(f\"Missing files:\")\n",
    "    if not os.path.exists(train_file):\n",
    "        print(f\"  - {train_file}\")\n",
    "    if not os.path.exists(val_file):\n",
    "        print(f\"  - {val_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "    \"\"\"Custom dataset for recipe generation using pre-formatted ChatML data.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer, max_length: int = 1024):\n",
    "        \"\"\"Initialize dataset.\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = self.load_data(data_path)\n",
    "        print(f\"Loaded {len(self.data):,} samples from {data_path}\")\n",
    "\n",
    "    def load_data(self, data_path: str) -> List[dict]:\n",
    "        \"\"\"Load ChatML data from JSONL file.\"\"\"\n",
    "        data = []\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line.strip()))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return dataset length.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get single item.\"\"\"\n",
    "        item = self.data[idx]\n",
    "        prompt = item['text']\n",
    "\n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encodings[\"input_ids\"].squeeze(),\n",
    "        }\n",
    "\n",
    "# Preview sample data\n",
    "with open('data/recipes_train_chat.jsonl', 'r') as f:\n",
    "    sample = json.loads(f.readline())\n",
    "    print(\"\\nüìù Sample data:\")\n",
    "    print(f\"Text length: {len(sample['text'])} chars\")\n",
    "    print(f\"Scenario: {sample.get('scenario', 'N/A')}\")\n",
    "    print(f\"\\nFirst 500 chars:\")\n",
    "    print(sample['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Load Model & Apply LoRA\n",
    "\n",
    "**A100 Optimizations:**\n",
    "- 4-bit quantization for memory efficiency\n",
    "- LoRA for parameter-efficient fine-tuning\n",
    "- Only ~0.28% of parameters trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA configuration\n",
    "print(\"Applying LoRA configuration...\\n\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,                    # LoRA rank\n",
    "    lora_alpha=32,           # LoRA alpha (scaling factor)\n",
    "    lora_dropout=0.05,       # Dropout for regularization\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n‚úÖ LoRA applied! Only 0.28% of parameters will be trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training Setup\n",
    "\n",
    "**A100-Optimized Settings:**\n",
    "- Batch size: 8 (utilize A100 memory)\n",
    "- Gradient accumulation: 2 (effective batch = 16)\n",
    "- No gradient checkpointing (speed priority)\n",
    "- FP16 mixed precision\n",
    "- Expected speed: 0.5-0.8 it/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\\n\")\n",
    "\n",
    "train_dataset = RecipeDataset(\n",
    "    \"data/recipes_train_chat.jsonl\",\n",
    "    tokenizer,\n",
    "    max_length=1024  # Optimized for speed/quality balance\n",
    ")\n",
    "\n",
    "val_dataset = RecipeDataset(\n",
    "    \"data/recipes_val_chat.jsonl\",\n",
    "    tokenizer,\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"Train samples: {len(train_dataset):,}\")\n",
    "print(f\"Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "# Calculate training parameters\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 2\n",
    "num_epochs = 3\n",
    "effective_batch_size = batch_size * gradient_accumulation_steps\n",
    "total_steps = (len(train_dataset) // effective_batch_size) * num_epochs\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"Per-device batch size: {batch_size}\")\n",
    "print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {effective_batch_size}\")\n",
    "print(f\"Total epochs: {num_epochs}\")\n",
    "print(f\"Total training steps: {total_steps:,}\")\n",
    "print(f\"\\n‚è±Ô∏è Estimated time: 45-60 minutes on A100\")\n",
    "print(f\"üí∞ Estimated cost: $0.83 - $1.10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# A100-optimized training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3b_recipe_lora\",\n",
    "    \n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,        # Optimized for A100\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,        # Effective batch = 16\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    # Speed optimization\n",
    "    fp16=True,                             # Mixed precision\n",
    "    gradient_checkpointing=False,          # Disable for speed\n",
    "    optim=\"adamw_torch\",                   # Fast optimizer\n",
    "    \n",
    "    # Logging & evaluation\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # System\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized!\")\n",
    "print(\"\\nüöÄ Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Start Training\n",
    "\n",
    "**Expected Performance on A100:**\n",
    "- Speed: ~0.5-0.8 it/s (iterations per second)\n",
    "- Time: 45-60 minutes\n",
    "- Cost: $0.83 - $1.10\n",
    "\n",
    "**Progress will be shown every 10 steps**\n",
    "**Validation runs every 100 steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "output_dir = \"./llama3b_recipe_lora_final\"\n",
    "\n",
    "print(f\"üíæ Saving model to {output_dir}...\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\n‚úÖ Model saved!\")\n",
    "print(\"\\nüìÅ Saved files:\")\n",
    "!ls -lh {output_dir}\n",
    "\n",
    "# Get model size\n",
    "!du -sh {output_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "print(\"üß™ Testing the fine-tuned model...\\n\")\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    {\n",
    "        \"name\": \"Korean Recipe Test\",\n",
    "        \"prompt\": \"\"\"<|im_start|>system\n",
    "You are a recipe generation AI that creates recipes based on user inventory and preferences.<|im_end|>\n",
    "<|im_start|>user\n",
    "I have chicken, rice, onion, and garlic. I want a Korean recipe.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Vegan Recipe Test\",\n",
    "        \"prompt\": \"\"\"<|im_start|>system\n",
    "You are a recipe generation AI that creates recipes based on user inventory and preferences.<|im_end|>\n",
    "<|im_start|>user\n",
    "I have tofu, broccoli, and soy sauce. I want a vegan recipe.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for test in test_prompts:\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test: {test['name']}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    inputs = tokenizer(test['prompt'], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    \n",
    "    # Extract just the assistant response\n",
    "    if \"<|im_start|>assistant\" in response:\n",
    "        assistant_response = response.split(\"<|im_start|>assistant\")[-1]\n",
    "        assistant_response = assistant_response.replace(\"<|im_end|>\", \"\").strip()\n",
    "        print(assistant_response)\n",
    "    else:\n",
    "        print(response)\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Download Model\n",
    "\n",
    "Download the fine-tuned LoRA adapter to use locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model for download\n",
    "print(\"üì¶ Zipping model files...\")\n",
    "!zip -r llama3b_recipe_lora_final.zip llama3b_recipe_lora_final/\n",
    "\n",
    "print(\"\\n‚úÖ Model zipped!\")\n",
    "!ls -lh llama3b_recipe_lora_final.zip\n",
    "\n",
    "print(\"\\nüì• Download the file using one of these methods:\")\n",
    "print(\"1. Right-click on llama3b_recipe_lora_final.zip in JupyterLab file browser ‚Üí Download\")\n",
    "print(\"2. Use SCP from your local machine:\")\n",
    "print(\"   scp ubuntu@<lambda-ip>:~/llama3b_recipe_lora_final.zip .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä View Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir llama3b_recipe_lora/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the model (`llama3b_recipe_lora_final.zip`)\n",
    "2. Stop the Lambda instance to avoid charges\n",
    "3. Use the model locally for inference\n",
    "\n",
    "### Cost Summary:\n",
    "- Training time: ~45-60 minutes\n",
    "- A100 rate: $1.10/hour\n",
    "- Total cost: **$0.83 - $1.10**\n",
    "\n",
    "### Model Usage:\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./llama3b_recipe_lora_final\")\n",
    "\n",
    "# Generate recipe\n",
    "# ... (use as shown in test section)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
