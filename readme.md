# ğŸ¥¢ PantryPilot â€“ Data Pipeline Documentation

### Group 16 Â· Personalized Grocery Forecasting & Constraint-Aware Recipe Assistant

---

## ğŸ“˜ Overview

**PantryPilot** is a personalized grocery management and recipe recommendation system that helps users maintain their kitchen inventory, plan meals intelligently, and avoid ingredient shortages.

This document outlines the **Data Pipeline** component of the project â€” responsible for ingestion, validation, transformation, and monitoring of all pantry-related datasets.

The pipeline ensures that every dataset (inventory, purchase history, and receipt coordination data) is:

* Ingested from the central **NeonDB** database
* Automatically validated using **Great Expectations**
* Transformed into standardized, ready-to-use tables
* Monitored for anomalies (low stock, expiry)
* Version-controlled using **DVC** for reproducibility

---

## ğŸ§± Architecture

```
[Synthetic Data Generation]
 Data_Pipeline/data/scripts/synthetic_generate.py
 â†’ Generates diverse, unbiased food items (Western + Non-Western cuisines)
       â”‚
       â–¼
Neon Database (PostgreSQL)
â”œâ”€â”€ inventory (from synthetic data)
â”œâ”€â”€ purchase_history (from synthetic data)
â””â”€â”€ cord_dataset (receipt images)
       â”‚
       â–¼
[Ingestion Layer]
 Data_Pipeline/scripts/ingest_neon.py
       â”‚
       â–¼
[Validation Layer]
 Data_Pipeline/scripts/validate_data.py (Great Expectations)
       â”‚
       â–¼
[Transformation Layer]
 Data_Pipeline/scripts/transform_data.py + Data_Pipeline/scripts/utils_pint.py
       â”‚
       â–¼
[Monitoring & Alerts]
 Data_Pipeline/scripts/update_anomalies.py
       â”‚
       â–¼
[Versioning]
 Git + DVC (raw / processed / alerts / receipts / synthetic_data)
```

---

## ğŸ§‰ Pipeline Components

### 0. Synthetic Data Generation

**Script:** `Data_Pipeline/data/scripts/synthetic_generate.py`
**Goal:** Generate realistic, diverse grocery data for testing and development.

**Key Features:**
* **Bias Mitigation:** Includes both Western and Non-Western food items to prevent cultural bias in the dataset
* **Diverse Food Items:** Rice, kimchi, tofu, soy sauce, ginger, and other Asian staples alongside Western ingredients
* **Realistic Attributes:** Categories, expiry dates, storage types, nutritional tags, and purchase patterns
* **Configurable:** 20 users, 50 items per user, 300 purchases per user

**Output:**
* `Data_Pipeline/data/synthetic_data/pantrypilot_inventory_u20_i60_shared_ids.csv`
* `Data_Pipeline/data/synthetic_data/pantrypilot_purchase_u20_i60_shared_ids.csv`

**Upload to NeonDB:** Data is uploaded to PostgreSQL database for pipeline consumption.

---

### 1. Ingestion Layer

**Script:** `Data_Pipeline/scripts/ingest_neon.py`
**Goal:** Extract structured data from NeonDB and store as snapshots in `Data_Pipeline/data/raw/`.

**Datasets:**

* `inventory.csv` (synthetic data uploaded to NeonDB)
* `purchase_history.csv` (synthetic data uploaded to NeonDB)
* `cord_dataset.csv` (receipt images dataset)

**Output Path:** `Data_Pipeline/data/raw/`

---

### 2. Validation Layer

**Script:** `Data_Pipeline/scripts/validate_data.py`
**Framework:** Great Expectations

**Purpose:**

* Validate schema, nulls, and logical rules
* Generate interactive HTML quality reports

**Outputs:**

* `Data_Pipeline/great_expectations/uncommitted/data_docs/local_site/index.html`
* `Data_Pipeline/reports/validation_summary.csv`

**Example Output:**

```
[VALIDATION] inventory.csv â†’ PASS âœ…
[VALIDATION] purchase_history.csv â†’ FAIL âŒ
```

Failures intentionally left to demonstrate detection of data issues.

---

### 3. Transformation Layer

**Scripts:** `Data_Pipeline/scripts/transform_data.py`, `Data_Pipeline/scripts/utils_pint.py`
**Goal:** Normalize units and engineer useful features.

**Key Steps:**

* Convert units to canonical form using **Pint** (g/ml/pcs)
* Derive: `stock_value = qty Ã— unit_cost`, `is_low_stock` flag
* Calculate per-unit prices in purchases
* Save standardized data to `Data_Pipeline/data/processed/`

---

### 4. Monitoring & Alerts

**Script:** `Data_Pipeline/scripts/update_anomalies.py`
**Purpose:** Identify low-stock or expired products and log them.

**Output:** `Data_Pipeline/data/alerts/alerts.csv`

| item_name | issue_type | quantity | expiry_date |
| --------- | ---------- | -------- | ----------- |
| Milk      | Expired    | 1        | 2025-09-15  |
| Rice      | Low Stock  | 0.45 kg  | â€”           |

---

### 5. Versioning & Reproducibility

**Tools:** Git + DVC

**Tracked Folders:**

```
Data_Pipeline/data/raw/
Data_Pipeline/data/processed/
Data_Pipeline/data/alerts/
```

**Commands:**

```bash
dvc init
git add .dvc .dvcignore
dvc add Data_Pipeline/data/raw Data_Pipeline/data/processed Data_Pipeline/data/alerts
git commit -m "Track datasets with DVC"
```

**Verification:**

```
dvc status â†’ Data and pipelines are up to date.
```

---

### 6. Orchestration (Future Scope)

The modular design supports future integration with **Prefect** or **Airflow** for automated DAG execution (Ingest â†’ Validate â†’ Transform â†’ Alert) and scheduled runs on cloud environments like Databricks or Azure ML.

---

## ğŸ§® Folder Structure

```
PantryPilot/
â”œâ”€â”€ Data_Pipeline/
â”‚   â”œâ”€â”€ alerts.dvc                 # DVC tracked alerts directory
â”‚   â”œâ”€â”€ processed.dvc              # DVC tracked processed directory
â”‚   â”œâ”€â”€ raw.dvc                    # DVC tracked raw directory
â”‚   â”œâ”€â”€ airflow/                   # Airflow DAG definitions
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ alerts/                # Generated alerts CSVs
â”‚   â”‚   â”œâ”€â”€ processed/             # Transformed datasets
â”‚   â”‚   â”œâ”€â”€ raw/                   # Snapshot CSVs from Neon
â”‚   â”‚   â”œâ”€â”€ receipts/              # CORD receipt dataset
â”‚   â”‚   â”œâ”€â”€ scripts/               # Synthetic data utilities
â”‚   â”‚   â”‚   â””â”€â”€ synthetic_generate.py
â”‚   â”‚   â””â”€â”€ synthetic_data/        # Generated synthetic datasets
â”‚   â”œâ”€â”€ docs/                      # Pipeline-specific documentation
â”‚   â”œâ”€â”€ great_expectations/        # GE configuration and artifacts
â”‚   â”œâ”€â”€ reports/                   # Validation and profiling outputs
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ bias_check.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ ingest_neon.py
â”‚   â”‚   â”œâ”€â”€ logging_conf.py
â”‚   â”‚   â”œâ”€â”€ profile_stats.py
â”‚   â”‚   â”œâ”€â”€ transform_data.py
â”‚   â”‚   â”œâ”€â”€ update_anomalies.py
â”‚   â”‚   â”œâ”€â”€ utils_pint.py
â”‚   â”‚   â”œâ”€â”€ validate_data.py
â”‚   â”‚   â””â”€â”€ receipts/
â”‚   â”‚       â”œâ”€â”€ create_url_csv.py
â”‚   â”‚       â”œâ”€â”€ upload_to_gcs.sh
â”‚   â”‚       â””â”€â”€ upload_to_neon.py
â”‚   â”œâ”€â”€ tests/                     # Pytest suites
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ dvc.yaml
â”‚   â””â”€â”€ readme.md
â”œâ”€â”€ DataCard/                      # Model and data documentation
â”œâ”€â”€ docs/                          # Global documentation
â”œâ”€â”€ flows/                         # Prefect workflow definitions
â””â”€â”€ venv/                          # Local Python virtual environment
```

---

## ğŸ§° Tools and Technologies

| Category               | Tool                       | Purpose                        |
| ---------------------- | -------------------------- | ------------------------------ |
| Database               | **NeonDB (PostgreSQL)**    | Centralized data storage       |
| Data Handling          | **pandas**, **SQLAlchemy** | ETL and transformation         |
| Validation             | **Great Expectations**     | Schema and data quality checks |
| Transformation         | **Pint**                   | Unit conversions               |
| Versioning             | **Git**, **DVC**           | Code and data reproducibility  |
| Monitoring             | **Python scripts**         | Alert generation               |
| Orchestration (Future) | **Prefect**                | DAG automation                 |

---

## ğŸ§¾ Running the Pipeline

### 1. Setup

```bash
git clone https://github.com/abhikothari091/PantryPilot.git
cd PantryPilot
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate
pip install --upgrade pip
pip install -r Data_Pipeline/requirements.txt
cd Data_Pipeline
```

### 2. End-to-End Execution (from `Data_Pipeline/`)

```bash
# Optional: refresh synthetic sources feeding Neon
python data/scripts/synthetic_generate.py

# Core pipeline
python -m scripts.ingest_neon
python -m scripts.validate_data
python -m scripts.transform_data
python -m scripts.update_anomalies

# Monitoring & bias analytics
python -m scripts.bias_check
python -m scripts.profile_stats

# Tests & tracking
pytest -q tests
dvc status
```

- Validation artefacts: `great_expectations/uncommitted/data_docs/local_site/index.html`
- Logged summary: `reports/validation_summary.csv`
- Alerts export: `data/alerts/alerts.csv`

### 3. (Optional) Airflow Sanity Check

```bash
airflow dags test pantry_pilot_dag 2025-01-01
```

---

## ğŸ“Š Outputs

| Stage          | Output                            | Description            |
| -------------- | --------------------------------- | ---------------------- |
| Ingestion      | `/Data_Pipeline/data/raw/*.csv`   | Raw tables from NeonDB |
| Validation     | GE HTML report                    | Data quality summary   |
| Transformation | `/Data_Pipeline/data/processed/*.csv` | Standardized data      |
| Monitoring     | `/Data_Pipeline/data/alerts/alerts.csv` | Alerts for anomalies   |
| Logging        | `/Data_Pipeline/reports/validation_summary.csv` | Validation status      |
| Versioning     | `.dvc` files                      | Data lineage metadata  |

---

## ğŸ§  Reflection and Learnings

This project provided hands-on experience with designing a **real-world, production-ready data pipeline**.

**Key Learnings:**

* **Data Bias Mitigation:** Synthetic data generation includes diverse cuisines (Western and Non-Western foods) to prevent cultural bias in ML models.
* **Great Expectations:** Ensured data integrity and schema consistency through automated validation.
* **DVC:** Enabled reproducible data versioning and efficient storage tracking.
* **Modular Design:** Promotes maintainability and future scalability across pipeline components.
* **Transformation Logic:** Using Pint reinforced data standardization best practices for unit conversions.
* **Intentional Validation Failures:** Demonstrated pipeline robustness and error detection capabilities.
* **Receipt Processing:** Integrated CORD dataset for real-world receipt image processing.

This pipeline now forms the **foundation** of the larger PantryPilot system, supporting the downstream Recipe Generator and Inventory Forecaster modules.

---

## ğŸš€ Future Enhancements

* Integrate Prefect for automated end-to-end orchestration.
* Configure DVC remote storage (Google Drive / DagsHub).
* Introduce APIs for real-time alerting and dashboarding.
* Enhance Great Expectations suites with dynamic thresholds and schema evolution.

---
