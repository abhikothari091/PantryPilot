# Model Development – LLM Evaluation & Bias Analysis

This document describes the model development and evaluation phase for PantryPilot's recipe generation LLM.

The goals of this phase are to:

1. Compare the base Llama 3.2 3B Instruct model against the LoRA‑fine‑tuned recipe model.
2. Evaluate how well each model follows:
   - The required JSON output schema
   - Dietary constraints (vegan, vegetarian, gluten‑free, dairy‑free, non‑veg, none)
   - Cuisine preferences
   - Use of the user's inventory
3. Perform a targeted bias / fairness check across cuisines and dietary preferences.
4. Produce reproducible metrics and reports that can be referenced in the main project README and final report.
5. Integrate this into the overall PantryPilot pipeline, including CI smoke tests.

All code for this phase lives under:

```
model_development/
  └── llm_eval/
        ├── __init__.py
        ├── config.py
        ├── datasets.py
        ├── metrics.py
        ├── run_eval.py
        ├── bias_eval.py
        ├── analyze_results.py
        ├── data/                  # logical home for eval-related data
        └── reports/               # JSON + CSV outputs
              ├── eval_*.json
              ├── eval_summary_*.csv
              └── bias_report.csv
```

The evaluation datasets themselves are stored under the data pipeline module:

```
data_pipeline/data/recipes/
  ├── recipes_test.jsonl   # main evaluation set
  └── val_bias.json        # bias / fairness eval set
```

---

## 1. Models & Data

### 1.1 Models

We compare two models:

1. **Base model**
   - `meta-llama/Llama-3.2-3B-Instruct`
   - Loaded from Hugging Face using `AutoModelForCausalLM.from_pretrained`.

2. **Fine‑tuned model (LoRA)**
   - Base: the same `meta-llama/Llama-3.2-3B-Instruct`
   - LoRA adapter directory: `models/llama3b_lambda_lora/`
   - Contains `adapter_model.safetensors`, `adapter_config.json`, and other PEFT artifacts.
   - Trained by a teammate on ~11.8k cleaned synthetic recipes using PEFT / LoRA.

**Important:**
- The LoRA adapter is not tracked in Git. It must be downloaded separately (e.g., from Drive or a model registry) and placed under:
  ```
  models/llama3b_lambda_lora/
  ```

- The base model is fetched from Hugging Face. Access may require an HF token for gated models:
  ```bash
  export HF_TOKEN="<your-hf-token>"  # or use `huggingface-cli login`
  ```

### 1.2 Evaluation Datasets

We use two datasets for this phase.

#### 1.2.1 Main test set – `recipes_test.jsonl`

**Location:**
```
data_pipeline/data/recipes/recipes_test.jsonl
```

**Format:** JSONL, one object per line. Each object maps to the `RecipeTestExample` dataclass:

```python
@dataclass
class RecipeTestExample:
    user_inventory: List[str]
    preference: Optional[str]        # "vegan", "vegetarian", "gluten-free", "dairy-free", "non-veg", "none"
    cuisine: Optional[str]           # e.g. "Italian", "Chinese", "Mexican", ...
    user_request: Optional[str]      # free text: "quick dinner", "high protein", etc.
    requested_ingredients: List[str]
    gold_output: Dict[str, Any]      # synthetic ground-truth recipe JSON
    scenario: str                    # scenario tag
    generated_at: str                # timestamp
```

The test set is generated by the synthetic pipeline and held out from training. It contains ~1200 examples covering multiple scenarios and constraint combinations.

#### 1.2.2 Bias set – `val_bias.json`

**Location:**
```
data_pipeline/data/recipes/val_bias.json
```

**Format:** a JSON array of hand‑crafted prompts. Each element is a small dict with:

```json
{
  "user_inventory": ["tofu", "spinach", "rice"],
  "preference": "vegan",
  "cuisine": "Chinese",
  "user_request": "Quick weekday dinner"
}
```

These ~29 examples are designed to:
- Cover multiple cuisines: Italian, Chinese, Mexican, Indian, Japanese, Korean, Mediterranean, Middle Eastern, Thai, American, Spanish, etc.
- Cover multiple dietary preferences: vegan, vegetarian, gluten‑free, dairy‑free, non‑veg, none.
- Use realistic inventory combinations and textual requests.

Bias analysis does not use gold outputs; it focuses on how the model behaves across slices (diet × cuisine) using the same metrics as the main eval.

### 1.3 Model Artifacts & Storage

Model artifacts are handled as follows:

- **Base model:** always loaded from Hugging Face via `BASE_MODEL_NAME`.
- **LoRA adapter:**
  - Canonical local path: `models/llama3b_lambda_lora/`.
  - Canonical remote artifact path (for sharing within the team):
    ```
    gs://pantrypilot-dvc-storage/data/models/llama3b_lambda_lora.zip
    ```
  - The GCS bucket is also used as the default DVC remote (`myremote`) for larger artifacts.

The repository `.gitignore` is configured to exclude large or licensed weights (see section 8).

---

## 2. Prompting & Generation

### 2.1 System Prompt

All evaluation uses a consistent ChatML‑style prompt that matches the backend's serving format.

`run_eval.py` defines a shared `SYSTEM_PROMPT` that:
- Identifies the assistant as RecipeGen, a recipe generation AI.
- Enforces JSON‑only responses:
  - Exactly one JSON object
  - No markdown, backticks, or stray text
  - Valid JSON syntax (double quotes, lowercase true/false/null)
- Defines the conceptual inputs:
  - inventory
  - optional dietary_preference
  - optional cuisine
  - optional user_request
- Specifies the exact JSON schema:

```json
{
  "status": "ok",
  "missing_ingredients": ["string"],
  "recipe": {
    "name": "string",
    "cuisine": "string",
    "culinary_preference": "string",
    "time": "string",
    "main_ingredients": ["string"],
    "steps": "string",
    "note": "string or null"
  },
  "shopping_list": ["string"]
}
```

- Includes strict rules, such as:
  - `missing_ingredients` and `shopping_list` must not exceed 8 items.
  - Avoid giant, spammy ingredient lists.
  - Respect dietary preferences (e.g. no meat/dairy for vegan).
  - Match requested cuisine when specified.

### 2.2 ChatML Prompt Construction

`build_chatml_prompt(example: RecipeTestExample)` converts each test example into a user message:

```
<|im_start|>system
{SYSTEM_PROMPT}
<|im_end|>
<|im_start|>user
Inventory: rice, onion, lemon.
Dietary preference: vegan.
Cuisine: Chinese.
Request: Quick stir-fry using only what I have.
<|im_end|>
<|im_start|>assistant
```

The model generates only the assistant continuation, which should be the JSON object described above.

---

## 3. Core Scripts

### 3.1 `config.py`

Central configuration for the evaluation code. Key fields include:
- `PROJECT_ROOT`: repository root as a Path.
- `BASE_MODEL_NAME`: `"meta-llama/Llama-3.2-3B-Instruct"`.
- `LORA_ADAPTER_DIR`: `models/llama3b_lambda_lora/`.
- `MAX_NEW_TOKENS`: maximum generation length for recipes (e.g. 256).
- `TEMPERATURE_GRID`: default temperature list (e.g. `[0.7]`).

### 3.2 `datasets.py`

**Responsibilities:**
- Defines the `RecipeTestExample` dataclass.
- Implements `load_recipes_test()`:
  - Reads `recipes_test.jsonl` from the data pipeline folder.
  - Converts each JSON line into a `RecipeTestExample` instance.
  - Used by `run_eval.py` as the main evaluation set.

A thin wrapper around the pipeline's recipe data provides a clean interface to the evaluation tooling.

### 3.3 `metrics.py`

This module defines how we parse outputs and score each example.

**Key functions:**
- `parse_model_json(raw: str) -> Tuple[Dict, bool]`
  - Attempts to extract a JSON object from the raw model text.
  - Handles minor formatting issues (e.g. trailing special tokens).
  - Returns `(parsed_json, is_valid_bool)`.

- `compute_example_metrics(example: RecipeTestExample, parsed: Dict, valid: bool) -> ExampleMetrics`
  - If `valid` is False, returns metrics with `json_valid_rate = 0.0` and zeros for the others.
  - Otherwise computes per‑example metrics including:
    - `json_valid_rate`: 1.0 if parsed, 0.0 otherwise.
    - `diet_match_rate`: 1.0 if the recipe's ingredients respect the requested dietary preference; 0.0 if violated.
    - `constraint_violation_rate`: 1.0 if constraints are violated, 0.0 if fully respected.
    - `cuisine_match_rate`: 1.0 if the recipe cuisine matches the requested cuisine (if any), else 0.0.
    - `inventory_coverage`: fraction of main_ingredients that come from the user inventory.

- `aggregate_metrics(list_of_ExampleMetrics) -> Dict[str, float]`
  - Aggregates per‑example metrics into means across the dataset.

### 3.4 `run_eval.py`

Main script for evaluation on the full test set.

**Typical usage (from repo root):**

```bash
# Quick check on a small subset
python -m model_development.llm_eval.run_eval \
  --max-examples 3 \
  --temperatures 0.7

# Benchmark run on a subset
python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7
```

**Behavior:**

1. **Device & dtype selection**
   ```python
   device = pick_device()  # cuda > mps > cpu
   dtype = torch.float16 if device.type != "cpu" else torch.float32
   ```

2. Load test examples via `load_recipes_test()`.

3. Load tokenizer once using `AutoTokenizer.from_pretrained(BASE_MODEL_NAME)`.

4. For each `model_kind` in `["base", "lora"]`:
   - Load either the base model or base+LoRA:
     ```python
     if model_kind == "base":
         model = AutoModelForCausalLM.from_pretrained(
             BASE_MODEL_NAME,
             torch_dtype=dtype,
         )
     else:
         base_for_lora = AutoModelForCausalLM.from_pretrained(
             BASE_MODEL_NAME,
             torch_dtype=dtype,
         )
         model = PeftModel.from_pretrained(base_for_lora, LORA_ADAPTER_DIR)
     ```
   - Move to device and set `eval()`.

5. For each temperature in `TEMPERATURE_GRID` (usually 0.7):
   - Call `eval_model(...)`:
     - Loops over examples (up to `--max-examples`).
     - Uses `generate_single(...)` to produce raw text.
     - Parses JSON and computes `ExampleMetrics`.
     - Aggregates metrics with `aggregate_metrics`.

6. After finishing both models, saves results to:
   ```
   model_development/llm_eval/reports/
     ├── eval_YYYYMMDD_HHMMSS.json
     └── eval_summary_YYYYMMDD_HHMMSS.csv
   ```

The CSV contains one row per run, e.g. `base_t0.7`, `lora_t0.7` with the aggregated metrics.

To avoid MPS OOM on laptops, the script loads one model at a time and frees it between runs.

### 3.5 `bias_eval.py`

Script for targeted bias / fairness analysis on `val_bias.json`.

**Usage:**
```bash
python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30
```

**Behavior:**

1. Loads the bias dataset from `data_pipeline/data/recipes/val_bias.json` using a helper that builds `RecipeTestExample` objects (using only the fields present in this file).

2. Evaluates both base and LoRA models using the same prompt and metrics as `run_eval.py`.

3. Groups results by `(model, preference, cuisine)`.

4. Aggregates using the same mean‑based logic as `aggregate_metrics`.

5. Writes:
   ```
   model_development/llm_eval/reports/bias_report.csv
   ```

Each row contains:
- `model` ("base" or "lora")
- `preference`
- `cuisine`
- `n` (number of examples in that slice)
- Mean metrics (`json_valid_rate`, `diet_match_rate`, `constraint_violation_rate`, `cuisine_match_rate`, `inventory_coverage_mean`).

### 3.6 `analyze_results.py`

Helper script to summarize and compare results.

**Usage:**
```bash
python -m model_development.llm_eval.analyze_results
```

**Typical behavior:**
- Finds the latest `eval_summary_*.csv` and loads it.
- Loads `bias_report.csv` if present.
- Prints human‑readable tables (e.g., markdown‑style) comparing:
  - Base vs LoRA on the main test set.
  - Base vs LoRA per preference–cuisine slice on the bias set.

These tables can be pasted into documentation or slides.

---

## 4. Example Results – Main Test Set

This section documents representative results from development runs on an Apple M3 Pro laptop with 18 GB RAM, using MPS acceleration and temperature = 0.7.

**Command used:**
```bash
python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7
```

**Resulting summary (from `eval_summary_*.csv`):**

- **base_t0.7**
  - `json_valid_rate`            ≈ 1.00
  - `diet_match_rate`            ≈ 0.43
  - `constraint_violation_rate`  ≈ 0.57
  - `cuisine_match_rate`         ≈ 1.00
  - `inventory_coverage_mean`    ≈ 0.70

- **lora_t0.7**
  - `json_valid_rate`            ≈ 1.00
  - `diet_match_rate`            ≈ 0.71
  - `constraint_violation_rate`  ≈ 0.29
  - `cuisine_match_rate`         ≈ 1.00
  - `inventory_coverage_mean`    ≈ 0.67

**Interpretation:**
- Both models produce valid JSON on essentially all examples for this subset.
- The LoRA model:
  - Substantially improves adherence to dietary constraints (`diet_match_rate` up from ~0.43 to ~0.71).
  - Reduces constraint violations (`constraint_violation_rate` down from ~0.57 to ~0.29).
- Cuisine matching is already strong (≈ 1.0) for both models.
- Inventory coverage is comparable; small differences are expected given randomness and synthetic data.

These numbers match the qualitative expectation from fine‑tuning: the LoRA adapter helps the model internalize the recipe JSON format and constraint logic encoded in the training data.

### 4.1 Temperature Sensitivity (Optional Analysis)

The evaluation code supports a temperature grid via `TEMPERATURE_GRID` in `config.py`. For quick sensitivity checks, the following pattern can be used:

```bash
python -m model_development.llm_eval.run_eval \
  --max-examples 10 \
  --temperatures 0.0 0.3 0.7
```

**Typical behavior observed:**

- **`temperature = 0.0` (greedy):**
  - Very stable JSON and constraints.
  - Less variety and sometimes more "safe" but repetitive recipes.

- **`temperature = 0.3`:**
  - Good balance of variability and stability.

- **`temperature = 0.7`:**
  - Slightly more creative outputs.
  - Small increase in risk of constraint violations on the base model.
  - LoRA mitigates most of this risk while preserving diversity.

For the final system, `0.7` was chosen as a good trade‑off for constrained recipe generation.

---

## 5. Bias Set Results – `bias_report.csv`

**Command used:**

```bash
python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30
```

The resulting `bias_report.csv` contains rows such as:

```csv
model,preference,cuisine,n,json_valid_rate,diet_match_rate,constraint_violation_rate,cuisine_match_rate,inventory_coverage_mean
base,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.75
base,vegan,Korean,1,1.0,0.0,1.0,1.0,1.0
base,dairy-free,Italian,1,1.0,0.0,1.0,1.0,1.0
base,gluten-free,Italian,1,0.0,,,,             # invalid JSON
...
lora,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.5
lora,vegan,Korean,1,1.0,1.0,0.0,1.0,1.0
lora,dairy-free,Italian,1,1.0,0.0,1.0,1.0,1.0
lora,gluten-free,Italian,1,1.0,1.0,0.0,1.0,1.0
...
```

**Key observations:**

1. **JSON validity**
   - LoRA achieves `json_valid_rate = 1.0` for all slices in this bias set.
   - The base model fails on at least one slice: gluten-free + Italian (invalid JSON output).

2. **Dietary constraint adherence**
   - The base model shows several slices with `diet_match_rate = 0.0` and `constraint_violation_rate = 1.0`, e.g.:
     - base, vegan, Korean
     - base, dairy-free, Italian
   - The LoRA model improves most of these slices to `diet_match_rate = 1.0`, `constraint_violation_rate = 0.0`, including gluten-free, Italian which is fixed.
   - A few difficult slices (e.g. certain dairy‑free prompts) still show limitations, which are documented as residual risk.

3. **Cuisine & inventory coverage**
   - `cuisine_match_rate` is consistently 1.0 for both models across most slices, indicating stable control of cuisine tags.
   - `inventory_coverage_mean` varies between ~0.6 and 1.0; there is no obvious systematic bias where certain cuisines or diets are ignored. LoRA sometimes uses inventory slightly more aggressively, but differences are small on this sample.

**Overall:**

The bias evaluation shows that the fine‑tuned LoRA model is:
- More reliable (always valid JSON on the bias set).
- More faithful to dietary constraints across cuisines.
- Still not perfect in edge cases (e.g. some dairy‑free prompts), which is explicitly acknowledged.

These results provide a concrete, quantitative story for the final report and demonstrate that the team considered both correctness and fairness.

---

## 6. How to Reproduce Locally

### 6.1 Prerequisites

- Python 3.10+
- Sufficient disk space (~8–10 GB) for the base model.
- Apple M‑series (MPS), CUDA GPU, or CPU (CPU works but is slower).
- Access to:
  - `data_pipeline/data/recipes/recipes_test.jsonl`
  - `data_pipeline/data/recipes/val_bias.json`
  - LoRA adapter folder: `models/llama3b_lambda_lora/`

### 6.2 Install Dependencies

From the repository root:

```bash
# Data pipeline + core libs
pip install -r data_pipeline/requirements.txt

# LLM-specific extras if needed
pip install transformers peft accelerate
```

Configure Hugging Face credentials if required:

```bash
export HF_TOKEN="<your-hf-token>"
```

### 6.3 Run the Main Eval

```bash
python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7
```

**Outputs:**
```
model_development/llm_eval/reports/
  ├── eval_YYYYMMDD_HHMMSS.json
  └── eval_summary_YYYYMMDD_HHMMSS.csv
```

### 6.4 Run the Bias Eval

```bash
python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30
```

**Outputs:**
```
model_development/llm_eval/reports/bias_report.csv
```

### 6.5 Optional: Summarize Results

```bash
python -m model_development.llm_eval.analyze_results
```

Copy the printed tables into the main report or slides as needed.

---

## 7. CI Integration

The repository includes a GitHub Actions workflow that provides a lightweight CI layer for the model code. At a high level, the workflow:

1. Checks out the repository.
2. Sets up Python and installs dependencies (data pipeline + model development).
3. Runs unit tests and basic checks (e.g., pytest under `data_pipeline/tests` and any tests under `model_development`).
4. Runs smoke tests for the LLM evaluation scripts with very small settings to keep CI fast, for example:

   ```bash
   python -m model_development.llm_eval.run_eval \
     --max-examples 2 \
     --temperatures 0.7

   python -m model_development.llm_eval.bias_eval \
     --temperature 0.7 \
     --max-examples 2
   ```

5. Ensures that:
   - Models can be loaded on CPU.
   - JSON validity parsing works.
   - Metric computation functions do not crash.

This CI step ensures that future code changes do not silently break evaluation or bias analysis logic.

---

## 8. Models & `.gitignore`

To avoid committing large or licensed weights, the repository is configured (or should be configured) with entries such as:

```
models/
**/llama3b_lambda_lora/
**/adapter_model.safetensors
**/adapter_config.json

huggingface/
.huggingface/
```

The model development README (this file) serves as the source of truth for:
- Where to place the LoRA adapter.
- Which base model to download.
- How to rerun the evaluations and bias analysis.

---

## 9. Limitations & Future Work

- **Eval set size:** For runtime reasons, most local runs in development used subsets of the test set (e.g., 20 examples). Extending to the full ~1200 examples is possible on sufficiently powerful hardware.

- **Bias set size:** The bias evaluation uses ~30 carefully designed examples. It is diagnostic, not statistically exhaustive. A larger structured bias suite would be a natural next step.

- **Edge cases:** Some difficult slices (e.g., certain dairy‑free recipes) still yield occasional violations even for the LoRA model. These are documented but not fully eliminated.

- **Deeper experiment tracking:** Current tracking is file‑based (JSON/CSV). Integrating MLflow or Weights & Biases could provide richer experiment management.

- **Endpoint deployment:** The current phase focuses on offline evaluation and bias analysis. The next logical step is to wrap the chosen LoRA model into a service (FastAPI/Flask or cloud endpoint) and plug it into the PantryPilot app.

Despite these limitations, the current setup provides a clear, reproducible, and metric‑driven comparison between the base and fine‑tuned models, and demonstrates good LLMOps practices for monitoring correctness and bias in an application‑specific setting.
