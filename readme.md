ğŸ¥¢ PantryPilot â€“ Data Pipeline & Model Development

Group 16 Â· Personalized Grocery Forecasting & Constraint-Aware Recipe Assistant

â¸»

ğŸ“˜ High-Level Overview

PantryPilot is a personalized grocery management and recipe recommendation system that helps users:
	â€¢	Maintain and monitor their kitchen inventory
	â€¢	Plan meals based on available ingredients and preferences
	â€¢	Avoid ingredient shortages through alerts and smart suggestions

The system is composed of two major technical pillars:
	1.	Data Pipeline (this repoâ€™s data_pipeline/)
	â€¢	Ingestion from NeonDB (PostgreSQL)
	â€¢	Validation with Great Expectations
	â€¢	Transformation & feature engineering
	â€¢	Monitoring & alerts for low-stock / expiry
	â€¢	Data versioning with DVC + remote storage
	2.	Model Development (this repoâ€™s model_development/)
	â€¢	Synthetic recipe data generation & cleaning (teammate 2)
	â€¢	LoRA fine-tuning of Llama 3.2 3B Instruct
	â€¢	FastAPI + React app for recipe generation
	â€¢	Local evaluation & benchmarking of base vs fine-tuned model
	â€¢	Bias-focused slice evaluation across cuisines & dietary preferences
	â€¢	(Assumed) CI checks for tests and formatting

This README describes both the data pipeline and the model development work, plus how they connect conceptually.

â¸»

ğŸ§± System Architecture

[Synthetic Data Generation for Inventory]
 data_pipeline/data/scripts/synthetic_generate.py
 â†’ Generate diverse pantry items (Western + Non-Western cuisines)
       â”‚
       â–¼
Neon Database (PostgreSQL)
â”œâ”€â”€ inventory (from synthetic_data)
â”œâ”€â”€ purchase_history (from synthetic_data)
â””â”€â”€ cord_dataset (receipt images metadata)
       â”‚
       â–¼
[Data Pipeline]
 data_pipeline/scripts/ingest_neon.py          # Ingestion from NeonDB
 data_pipeline/scripts/validate_data.py        # Great Expectations validation
 data_pipeline/scripts/transform_data.py       # Pint-based unit normalization
 data_pipeline/scripts/update_anomalies.py     # Low stock / expiry alerts
 DVC + Airflow orchestration
       â”‚
       â–¼
[Cleaned Inventory + History]
 data_pipeline/data/processed/*.csv
       â”‚
       â–¼
[Model Development]
 model_development/
  â”œâ”€â”€ (Teammate 2) Synthetic recipe generation + LoRA fine-tuning
  â”œâ”€â”€ backend/ + frontend/ (external repo) for the app
  â””â”€â”€ llm_eval/ (this project)
        â”œâ”€â”€ run_eval.py      # Base vs LoRA evaluation
        â”œâ”€â”€ metrics.py       # Parsing and metric computation
        â”œâ”€â”€ datasets.py      # Test data loader
        â”œâ”€â”€ bias_eval.py     # Bias slice evaluation
        â””â”€â”€ data/recipes_test.jsonl, val_bias.json

In a fully integrated version of PantryPilot, the **data pipeline outputs** would feed into the **model inference layer** to drive personalized recipe generation and inventory-aware suggestions.


â¸»

ğŸ§‰ Data Pipeline Components

0. Synthetic Inventory & Purchase Data

Script: data_pipeline/data/scripts/synthetic_generate.py
Goal: Generate realistic, diverse grocery data for development & testing.

Key Features:
	â€¢	Bias mitigation: Includes both Western and Non-Western food items
	â€¢	e.g. rice, kimchi, tofu, soy sauce, ginger, Indian spices, etc.
	â€¢	Realistic attributes:
	â€¢	Category (e.g., produce, dairy, pantry)
	â€¢	Expiry dates
	â€¢	Storage type (fridge, freezer, pantry)
	â€¢	Nutritional tags
	â€¢	Purchase patterns per user
	â€¢	Configurable scale:
	â€¢	~20 users
	â€¢	~50â€“60 items per user
	â€¢	~300 purchases per user

Outputs:
	â€¢	data_pipeline/data/synthetic_data/pantrypilot_inventory_u20_i60_shared_ids.csv
	â€¢	data_pipeline/data/synthetic_data/pantrypilot_purchase_u20_i60_shared_ids.csv

These synthetic CSVs are then uploaded to NeonDB and exposed as inventory and purchase_history tables.

â¸»

1. Ingestion Layer

Script: data_pipeline/scripts/ingest_neon.py
Goal: Extract structured data from NeonDB and store as snapshots under data_pipeline/data/raw/.

Datasets:
	â€¢	inventory.csv (synthetic inventory in NeonDB)
	â€¢	purchase_history.csv (synthetic purchase history in NeonDB)
	â€¢	cord_dataset.csv (receipt images metadata, for future OCR / VLM integration)

Output Path:
	â€¢	data_pipeline/data/raw/

ingest_neon.py connects via SQLAlchemy using DB_URL from scripts/config.py, runs a SELECT * on each table, and writes the results as CSV snapshots.

â¸»

2. Validation Layer (Great Expectations)

Script: data_pipeline/scripts/validate_data.py
Framework: Great Expectations

Purpose:
	â€¢	Validate schema and column types
	â€¢	Check for nulls and invalid values
	â€¢	Enforce logical rules (e.g., expiry date after today, non-negative quantities)
	â€¢	Generate interactive HTML quality reports

Outputs:
	â€¢	HTML docs: data_pipeline/great_expectations/uncommitted/data_docs/local_site/index.html
	â€¢	Summary CSV: data_pipeline/reports/validation_summary.csv

Example Outcome:

[VALIDATION] inventory.csv       â†’ PASS âœ…
[VALIDATION] purchase_history.csv â†’ FAIL âŒ (intentional data issue for demo)

Intentional failures are left in to demonstrate how the pipeline surfaces data quality problems.

â¸»

3. Transformation Layer

Scripts:
	â€¢	data_pipeline/scripts/transform_data.py
	â€¢	data_pipeline/scripts/utils_pint.py

Goal: Normalize all quantities and engineer useful features.

Key Steps (Inventory):
	â€¢	Normalize units to canonical form (g, ml, pcs) using Pint via utils_pint.to_canonical.
	â€¢	Compute stock_value = qty_canonical Ã— unit_cost.
	â€¢	Compute is_low_stock flag based on reorder_threshold.
	â€¢	Write cleaned inventory to data_pipeline/data/processed/inventory_cleaned.csv.

Key Steps (Purchase History):
	â€¢	Compute unit_price = price_total / quantity_purchased (with safe division).
	â€¢	Write cleaned purchase history to data_pipeline/data/processed/purchase_history_cleaned.csv.

â¸»

4. Monitoring & Alerts

Script: data_pipeline/scripts/update_anomalies.py
Goal: Identify low-stock or expired items and log them as alerts.

Output: data_pipeline/data/alerts/alerts.csv

Example schema:

item_name	issue_type	quantity	expiry_date
Milk	Expired	1	2025-09-15
Rice	Low Stock	0.45 kg	â€”

These alerts can later be wired into a UI or notification service.

â¸»

5. Versioning & Reproducibility (DVC + GCS)

Tools: Git + DVC + Google Cloud Storage

Tracked folders:

data_pipeline/data/raw/
data_pipeline/data/processed/
data_pipeline/data/alerts/

Typical workflow:

# Initialize DVC
cd data_pipeline
dvc init

# Configure remote
dvc remote add -d myremote gs://pantrypilot-dvc-storage/data

git add .dvc .dvcignore

dvc add data/raw data/processed data/alerts
dvc push  # Upload to GCS

git add data/*.dvc .dvc/config
git commit -m "Track datasets with DVC and GCS remote"

Verification commands:

dvc status   # Check if local and remote are in sync
dvc pull     # Download from GCS if needed

This ensures that every pipeline run is reproducible with a specific version of the raw/processed/alerts data.

â¸»

6. Orchestration with Airflow

DAG file: data_pipeline/airflow/dags/pantry_pilot_dag.py

Pipeline Flow:

ingest_neon â†’ validate_data â†’ transform_data â†’ detect_anomalies â†’ dvc_status

DAG Configuration:
	â€¢	DAG ID: pantrypilot_data_pipeline
	â€¢	Schedule: Currently manual; can be set to "0 6 * * *" for daily 6 AM runs.
	â€¢	Core tasks:
	1.	ingest_neon â€“ Extract from NeonDB
	2.	validate_data â€“ Run Great Expectations
	3.	transform_data â€“ Perform transformations
	4.	detect_anomalies â€“ Generate alerts
	5.	dvc_status â€“ Check DVC sync state

Example test run:

export AIRFLOW_HOME=$(pwd)/airflow
airflow db migrate

# Test the full DAG
airflow dags test pantrypilot_data_pipeline 2025-01-01


â¸»

ğŸ§® Project Folder Structure (Updated)

PantryPilot/
â”œâ”€â”€ data_pipeline/                      # Main data pipeline
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ dags/
â”‚   â”‚       â””â”€â”€ pantry_pilot_dag.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ alerts/
â”‚   â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ receipts/
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â””â”€â”€ synthetic_generate.py
â”‚   â”‚   â””â”€â”€ synthetic_data/
â”‚   â”œâ”€â”€ great_expectations/
â”‚   â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ bias_check.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ ingest_neon.py
â”‚   â”‚   â”œâ”€â”€ logging_conf.py
â”‚   â”‚   â”œâ”€â”€ profile_stats.py
â”‚   â”‚   â”œâ”€â”€ transform_data.py
â”‚   â”‚   â”œâ”€â”€ update_anomalies.py
â”‚   â”‚   â”œâ”€â”€ utils_pint.py
â”‚   â”‚   â””â”€â”€ validate_data.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ dvc.yaml
â”‚
â”œâ”€â”€ model_development/                  # Model dev & evaluation
â”‚   â”œâ”€â”€ llm_eval/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ run_eval.py
â”‚   â”‚   â”œâ”€â”€ bias_eval.py
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ recipes_test.jsonl      # Synthetic eval set from teammate 2
â”‚   â”‚   â”‚   â””â”€â”€ val_bias.json           # Hand-crafted bias prompts
â”‚   â”‚   â””â”€â”€ reports/
â”‚   â”‚       â”œâ”€â”€ eval_*.json
â”‚   â”‚       â”œâ”€â”€ eval_summary_*.csv
â”‚   â”‚       â””â”€â”€ bias_report.csv
â”‚   â””â”€â”€ (optionally) models/            # NOT checked into git (see .gitignore)
â”‚       â””â”€â”€ llama3b_lambda_lora/        # LoRA adapter (local only)
â”‚
â”œâ”€â”€ DataCard/                           # Data & model documentation
â”œâ”€â”€ docs/                               # Global docs (slides, notes, etc.)
â”œâ”€â”€ .dvc/                               # DVC configuration
â””â”€â”€ .gitignore                          # Includes model_development/models/

We explicitly ignore the model_development/models/ folder so that large model artifacts are not pushed to git. Instead, instructions are provided for downloading / placing them locally.

â¸»

ğŸ§° Tools & Technologies

Area	Tools / Libraries
Database	NeonDB (PostgreSQL), SQLAlchemy
Data handling	pandas
Validation	Great Expectations
Units & transforms	Pint
Orchestration	Airflow
Versioning	Git + DVC + GCS remote
LLM base model	meta-llama/Llama-3.2-3B-Instruct
Fine-tuning	LoRA (PEFT), Lambda Labs GPU (teammate 2)
Inference & eval	Hugging Face Transformers, PEFT, PyTorch
Frontend / backend	React + FastAPI + MongoDB (external app repo)
CI (assumed)	GitHub Actions (lint + tests)


â¸»

ğŸš€ How to Run the Data Pipeline (Local)

1. Setup

# Clone repository
git clone https://github.com/abhikothari091/PantryPilot.git
cd PantryPilot/data_pipeline

# Virtual environment
python -m venv data_pipeline_venv
source data_pipeline_venv/bin/activate  # Windows: data_pipeline_venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

Configure database:

cp .env.example .env
# Edit .env and fill in DATABASE_URL for NeonDB

2. Manual step-by-step run

# 1. Ingest
python -m scripts.ingest_neon

# 2. Validate
python -m scripts.validate_data

# 3. Transform
python -m scripts.transform_data

# 4. Alerts
python -m scripts.update_anomalies

# 5. Optional profiling
python -m scripts.bias_check
python -m scripts.profile_stats

# 6. Tests
pytest -q tests

Outputs to verify:
	â€¢	data/raw/*.csv â†’ raw snapshots
	â€¢	data/processed/*.csv â†’ cleaned tables
	â€¢	data/alerts/alerts.csv â†’ anomalies
	â€¢	great_expectations/uncommitted/data_docs/local_site/index.html â†’ validation report
	â€¢	reports/validation_summary.csv â†’ validation summary

3. Airflow DAG run

export AIRFLOW_HOME=$(pwd)/airflow
airflow db migrate

# Test DAG
airflow dags test pantrypilot_data_pipeline 2025-01-01

Expected:
	â€¢	All 5 tasks succeed
	â€¢	Same artifacts as manual run

â¸»

ğŸ§  Model Development: LLM Training & Evaluation

The model development work for PantryPilot focuses on recipe generation conditioned on inventory and preferences, with a strong emphasis on dietary constraint adherence and bias/coverage evaluation.

A. Synthetic Recipe Data & LoRA Fine-Tuning (Teammate 2)

This part is primarily implemented in a separate repository (RecipeGen-LLM), summarized here because our evaluation code depends on its outputs.

1. Synthetic data generation (Groq + Llama 3.1 8B)
	â€¢	Generate ~12,000 synthetic recipes across 6 realistic scenarios:
	â€¢	Full inventory usage
	â€¢	Pure dietary constraints (vegan, vegetarian, gluten-free, dairy-free)
	â€¢	Cuisine-specific (Italian, Chinese, Mexican, Indian, Japanese, Korean)
	â€¢	Combined constraints (e.g., vegan Italian)
	â€¢	User-requested ingredients (all present)
	â€¢	Missing/partial match scenarios
	â€¢	Use Groq API + Llama 3.1 8B for fast, cost-effective generation.
	â€¢	Enforce JSON output with fields: recipe name, cuisine, culinary_preference, time, main_ingredients, steps, note, shopping_list.

2. ChatML conversion & cleaning
	â€¢	Convert recipes into ChatML-style conversations:
	â€¢	system: Instructions for the recipe generator
	â€¢	user: Inventory + preference request
	â€¢	assistant: JSON object with recipe
	â€¢	Run validation:
	â€¢	Check vegan, vegetarian, gluten-free, dairy-free compliance
	â€¢	Remove recipes that violate constraints (e.g., honey in vegan, soy sauce in gluten-free)
	â€¢	Final cleaned dataset: ~11,850 recipes.

3. LoRA fine-tuning on Lambda Labs
	â€¢	Base model: meta-llama/Llama-3.2-3B-Instruct.
	â€¢	Fine-tuning method: LoRA via PEFT.
	â€¢	Typical config:
	â€¢	Rank r = 16, alpha = 32
	â€¢	Target modules: q_proj, k_proj, v_proj, o_proj
	â€¢	3 epochs, AdamW, cosine LR schedule
	â€¢	Output adapter folder (downloaded separately): models/llama3b_lambda_lora/.

This fine-tuned adapter is loaded by our llm_eval code to compare against the base model.

â¸»

B. LLM Evaluation: Base vs LoRA

All LLM evaluation logic lives under model_development/llm_eval/.

1. Config & datasets
	â€¢	config.py defines:
	â€¢	PROJECT_ROOT: path to repo root
	â€¢	BASE_MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
	â€¢	LORA_ADAPTER_DIR: local path to the downloaded LoRA adapter
	â€¢	MAX_NEW_TOKENS: generation length cap
	â€¢	TEMPERATURE_GRID: list of temperatures to evaluate
	â€¢	datasets.py:
	â€¢	Defines a RecipeTestExample dataclass
	â€¢	Implements load_recipes_test() to read recipes_test.jsonl
	â€¢	Each test example encodes:
	â€¢	user_inventory (list of ingredient names)
	â€¢	preference (vegan / vegetarian / gluten-free / dairy-free / non-veg / none)
	â€¢	cuisine
	â€¢	user_request (optional free-text request)

2. Prompting & generation
run_eval.py provides:
	â€¢	SYSTEM_PROMPT describing RecipeGen behavior:
	â€¢	Always output exactly one valid JSON object, no markdown
	â€¢	Specific schema:

{
  "status": "ok",
  "missing_ingredients": ["..."],
  "recipe": {
    "name": "...",
    "cuisine": "...",
    "culinary_preference": "...",
    "time": "...",
    "main_ingredients": ["..."],
    "steps": "Step 1. ...",
    "note": null
  },
  "shopping_list": ["..."]
}


	â€¢	Strict rules to stop the model from exploding missing_ingredients into huge lists.

	â€¢	build_chatml_prompt(example):
	â€¢	Constructs a ChatML conversation:

<|im_start|>system
...SYSTEM_PROMPT...
<|im_end|>
<|im_start|>user
Inventory: rice, onion, lemon.
Dietary preference: gluten-free.
Cuisine: Chinese.
Request: Quick dinner using mostly my pantry.
<|im_end|>
<|im_start|>assistant


	â€¢	generate_single(...):
	â€¢	Tokenizes the ChatML prompt
	â€¢	Calls model.generate(...) with controlled temperature and MAX_NEW_TOKENS
	â€¢	Strips everything after <|im_end|> in the decoded text

3. Metrics
metrics.py defines:
	â€¢	parse_model_json(raw_text):
	â€¢	Extract the first JSON object from the raw string
	â€¢	Return (parsed_json, is_valid_json).
	â€¢	compute_example_metrics(example, parsed, valid):
	â€¢	json_valid_rate (per example: 1 if valid JSON)
	â€¢	diet_match_rate: how often the recipe output respects the dietary preference
	â€¢	constraint_violation_rate: fraction of outputs that violate diet rules
	â€¢	cuisine_match_rate: how often recipe.cuisine matches expected cuisine
	â€¢	inventory_coverage: fraction of recipe main ingredients that come from the inventory
	â€¢	aggregate_metrics(per_example_metrics):
	â€¢	Compute simple averages across examples to report dataset-level metrics.

4. Running the evaluation
From the repo root:

# Example: run on 20 test examples with T=0.7 on MPS or CPU
python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7

What it does:
	â€¢	Picks device (CUDA â†’ MPS â†’ CPU)
	â€¢	Loads tokenizer once
	â€¢	Loads base model and evaluates at each temperature
	â€¢	Frees memory
	â€¢	Loads base+LoRA model and evaluates at each temperature
	â€¢	Writes:
	â€¢	model_development/llm_eval/reports/eval_YYYYMMDD_HHMMSS.json
	â€¢	model_development/llm_eval/reports/eval_summary_YYYYMMDD_HHMMSS.csv

Example result (20-example run, T=0.7):
	â€¢	Base model base_t0.7:
	â€¢	json_valid_rate: 1.0
	â€¢	diet_match_rate: ~0.43
	â€¢	constraint_violation_rate: ~0.57
	â€¢	cuisine_match_rate: 1.0
	â€¢	inventory_coverage_mean: ~0.70
	â€¢	LoRA model lora_t0.7:
	â€¢	json_valid_rate: 1.0
	â€¢	diet_match_rate: ~0.71
	â€¢	constraint_violation_rate: ~0.29
	â€¢	cuisine_match_rate: 1.0
	â€¢	inventory_coverage_mean: ~0.67

Interpretation:
	â€¢	Both models reliably output valid JSON (evaluation prompt is strong enough).
	â€¢	LoRA model significantly improves dietary constraint adherence and reduces violations.
	â€¢	Inventory usage stays high for both models, with a minor drop for LoRA that is acceptable given better respect of constraints.

â¸»

C. Bias Evaluation

Bias evaluation checks whether performance is consistent across dietary preferences and cuisines.

1. Bias dataset: val_bias.json
	â€¢	Location: data_pipeline/data/recipes/val_bias.json
	â€¢	Manually designed ~29 scenarios covering:
	â€¢	Preferences: vegan, vegetarian, gluten-free, dairy-free, non-veg, none
	â€¢	Cuisines: Italian, Chinese, Mexican, Indian, Japanese, Korean, American, Mediterranean, Middle Eastern, Thai, Spanish, etc.
	â€¢	Mixed cases: conflicting preferences, â€œnoneâ€ preference but explicit meat, etc.

Each entry is something like:

{
  "user_inventory": ["tofu", "rice", "broccoli"],
  "preference": "vegan",
  "cuisine": "Chinese",
  "user_request": "Quick weekday dinner using mostly pantry items"
}

2. Bias evaluation script: bias_eval.py
	â€¢	Loads bias dataset and converts it to RecipeTestExample objects
	â€¢	Evaluates both base and lora models at a fixed temperature (e.g., 0.7)
	â€¢	Computes metrics for each example using the same logic as metrics.py
	â€¢	Aggregates by slice: (model, preference, cuisine)
	â€¢	Writes a CSV report summarizing metrics per slice.

Run:

python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30

Output:
	â€¢	model_development/llm_eval/reports/bias_report.csv

Example CSV snippet:

model,preference,cuisine,n,json_valid_rate,diet_match_rate,constraint_violation_rate,cuisine_match_rate,inventory_coverage_mean
base,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.75
...
lora,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.50
...

Observations:
	â€¢	Both models maintain high JSON validity and high cuisine_match_rate across all slices.
	â€¢	The LoRA model consistently achieves diet_match_rate = 1.0 for almost all slices, including stricter ones like vegan / gluten-free.
	â€¢	The base model occasionally violates constraints in vegan / dairy-free / gluten-free slices (non-zero constraint_violation_rate).
	â€¢	Inventory coverage remains strong and roughly similar for both models across slices.

This gives us a defensible story about fairness and robustness across cuisines and dietary preferences.

â¸»

ğŸ” CI / Testing (Assumed Setup)

To keep things maintainable, we assume a simple CI pipeline (e.g., GitHub Actions) that runs on each push / PR:
	â€¢	pip install -r data_pipeline/requirements.txt
	â€¢	pytest -q data_pipeline/tests
	â€¢	python -m model_development.llm_eval.run_eval --max-examples 1 --temperatures 0.7 (smoke test)
	â€¢	Optional: linting (e.g., ruff or flake8)

This ensures that:
	â€¢	The data pipeline still runs end-to-end on a small sample
	â€¢	LLM evaluation code is at least syntactically and logically correct

(Actual CI YAML is not included here but can easily be added under .github/workflows/.)

â¸»

ğŸ§  Reflection & Learnings (End-to-End)

From a full MLOps perspective, this project demonstrates:
	1.	Data-centric pipeline design
	â€¢	Synthetic data generation to break the â€œno dataâ€ deadlock
	â€¢	Validation, transformation, and alerting as first-class citizens
	â€¢	DVC + GCS for reproducible datasets and lineage
	2.	Model development with local + cloud resources
	â€¢	High-volume synthetic recipe generation using Groq API
	â€¢	Parameter-efficient fine-tuning (LoRA) on a reasonably small model (3B)
	â€¢	Clean separation between training repo and evaluation / pipeline repo
	3.	Robust evaluation & bias analysis
	â€¢	Structured JSON output enforced via prompts and metrics
	â€¢	Automatic checks for dietary constraint adherence and cuisine correctness
	â€¢	Custom bias slice evaluation across cuisines and diets
	4.	Practical deployment considerations
	â€¢	Local evaluation & inference using MPS/CPU/GPU
	â€¢	Data pipeline ready to feed downstream services
	â€¢	CI hooks (assumed) to prevent regressions

Overall, PantryPilot moves from synthetic inventory data â†’ clean, validated tables â†’ LLM-based recipe generation with measured behavior across multiple user segments. This matches the goals of an LLMOps-style course project: not just training a model, but integrating it into a reproducible, observable, and evaluable system.