{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-3 Mini Fine-tuning V3 on Google Colab T4 GPU\n",
    "\n",
    "**Configuration:**\n",
    "- Model: microsoft/Phi-3-mini-4k-instruct\n",
    "- Dataset: 4,500 train / 500 valid (enriched with diverse meat/fish)\n",
    "- Batch size: 2\n",
    "- Learning rate: 5e-5\n",
    "- Epochs: 2\n",
    "- LoRA rank: 8, alpha: 16\n",
    "\n",
    "**Estimated time on T4**: 2-2.5 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (without trl to avoid compatibility issues)\n",
    "!pip install -q transformers datasets peft bitsandbytes accelerate tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Training Data\n",
    "\n",
    "Upload `train.jsonl` and `valid.jsonl` files using the file upload button on the left sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploaded files\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"Checking files...\")\n",
    "assert os.path.exists('train.jsonl'), \"Please upload train.jsonl\"\n",
    "assert os.path.exists('valid.jsonl'), \"Please upload valid.jsonl\"\n",
    "\n",
    "# Count samples\n",
    "with open('train.jsonl', 'r') as f:\n",
    "    train_count = sum(1 for _ in f)\n",
    "    \n",
    "with open('valid.jsonl', 'r') as f:\n",
    "    valid_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"‚úÖ Train samples: {train_count:,}\")\n",
    "print(f\"‚úÖ Valid samples: {valid_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model and Setup LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Model configuration\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "output_dir = \"./phi3-recipe-lora-v3\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration (V3 settings)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                    # LoRA rank\n",
    "    lora_alpha=16,          # LoRA alpha (reduced from 20)\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nüìä Trainable parameters: {trainable_params:,} ({100 * trainable_params / total_params:.3f}%)\")\n",
    "print(f\"üìä Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": \"train.jsonl\",\n",
    "        \"validation\": \"valid.jsonl\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Train: {len(dataset['train'])} samples\")\n",
    "print(f\"   Valid: {len(dataset['validation'])} samples\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample data:\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Text length: {len(sample['text'])} chars\")\n",
    "print(f\"First 200 chars: {sample['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = dataset[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing train dataset\",\n",
    ")\n",
    "\n",
    "tokenized_valid = dataset[\"validation\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"validation\"].column_names,\n",
    "    desc=\"Tokenizing validation dataset\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Tokenized train: {len(tokenized_train)} samples\")\n",
    "print(f\"‚úÖ Tokenized valid: {len(tokenized_valid)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments (V3 optimized)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    # Training schedule\n",
    "    num_train_epochs=2,              # Reduced from 3 (prevent overfitting)\n",
    "    per_device_train_batch_size=2,   # Batch size 2\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,   # Effective batch size = 4\n",
    "    \n",
    "    # Optimizer\n",
    "    learning_rate=5e-5,              # Conservative (prevent overfitting)\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"paged_adamw_8bit\",        # Memory efficient\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=400,                  # Must be multiple of eval_steps (200)\n",
    "    eval_strategy=\"steps\",           # Changed from evaluation_strategy\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Memory optimization\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    # Other\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"\\nüìä Training details:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Total steps: {len(dataset['train']) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM (not masked LM)\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting fine-tuning...\")\n",
    "print(\"=\"*70)\n",
    "print(\"‚è±Ô∏è  Estimated time: 2-2.5 hours on T4 GPU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "print(\"\\nüì¶ Adapter files saved:\")\n",
    "print(\"   - adapter_config.json\")\n",
    "print(\"   - adapter_model.safetensors\")\n",
    "print(\"   - tokenizer files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "test_prompt = \"\"\"<|system|>\n",
    "You are a creative recipe generator with access to the user's pantry inventory.\n",
    "\n",
    "Available ingredients in pantry:\n",
    "- tomato (vegetable)\n",
    "- onion (vegetable)\n",
    "- garlic (vegetable)\n",
    "- chicken breast (meat) [FORBIDDEN for vegetarian]\n",
    "- olive oil (oil)\n",
    "- salt (seasoning)\n",
    "- pepper (seasoning)\n",
    "\n",
    "User dietary preference: vegetarian\n",
    "\n",
    "Instructions:\n",
    "1. Based on the user's request, select appropriate ingredients from the available inventory\n",
    "2. IMPORTANT: Respect dietary restrictions - do NOT select meat/fish for vegetarian recipes\n",
    "3. Generate a complete, practical recipe with clear steps<|end|>\n",
    "<|user|>\n",
    "I want a healthy vegetarian dinner<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"\\nüß™ Test Generation:\")\n",
    "print(\"=\"*70)\n",
    "print(generated_text)\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if it correctly avoided chicken breast\n",
    "if \"chicken\" not in generated_text.split(\"<|assistant|>\")[1].lower():\n",
    "    print(\"\\n‚úÖ SUCCESS: Model correctly avoided chicken breast for vegetarian recipe!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Model selected chicken for vegetarian recipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Trained Adapter\n",
    "\n",
    "Zip and download the adapter files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the adapter\n",
    "!zip -r phi3-recipe-lora-v3.zip {output_dir}\n",
    "\n",
    "print(\"‚úÖ Adapter zipped: phi3-recipe-lora-v3.zip\")\n",
    "print(\"\\nüì• Download the zip file using the file browser on the left\")\n",
    "\n",
    "# Show file size\n",
    "import os\n",
    "size_mb = os.path.getsize('phi3-recipe-lora-v3.zip') / (1024 * 1024)\n",
    "print(f\"\\nüì¶ File size: {size_mb:.2f} MB\")\n",
    "print(\"\\nüí° After downloading, extract and use the adapter on your Mac M4 Pro with MLX!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
