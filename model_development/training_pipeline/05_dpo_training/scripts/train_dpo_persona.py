"""
DPO (Direct Preference Optimization) í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel, LoraConfig, get_peft_model
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset
import yaml
import argparse
from pathlib import Path


def train_dpo_persona(
    base_model_path: str,
    adapter_path: str,
    persona_id: str,
    train_file: str,
    output_dir: str
):
    """
    í˜ë¥´ì†Œë‚˜ë³„ DPO í•™ìŠµ
    """
    print(f"\n{'='*60}")
    print(f"Training DPO model for: {persona_id}")
    print(f"{'='*60}\n")

    # 1. ëª¨ë¸ ë¡œë“œ (ê¸°ì¡´ fine-tuned ëª¨ë¸ ì‚¬ìš©)
    print("Loading base model and adapter...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    tokenizer.pad_token = tokenizer.eos_token

    # 4-bit ì–‘ìí™” ì„¤ì •
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True
    )

    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        quantization_config=bnb_config,
        torch_dtype=torch.float16,
        device_map={"": 0}  # ëª¨ë“  ë ˆì´ì–´ë¥¼ GPU 0ì— ë°°ì¹˜
    )

    # ê¸°ì¡´ LoRA adapter ë¡œë“œ
    model = PeftModel.from_pretrained(base_model, adapter_path)

    # LoRA íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
    model.print_trainable_parameters()  # ë””ë²„ê¹…ìš©
    for name, param in model.named_parameters():
        if 'lora' in name.lower():
            param.requires_grad = True

    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"Loading dataset from {train_file}...")
    dataset = load_dataset('json', data_files=train_file, split='train')

    # 80/20 train/eval split
    dataset = dataset.train_test_split(test_size=0.2, seed=42)

    print(f"Train samples: {len(dataset['train'])}")
    print(f"Eval samples: {len(dataset['test'])}")

    # 3. DPO í•™ìŠµ ì„¤ì •
    training_args = DPOConfig(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=1,  # Reduced for L4 GPU memory
        per_device_eval_batch_size=1,   # Reduced for L4 GPU memory
        gradient_accumulation_steps=8,  # Effective batch = 8

        learning_rate=5e-5,  # DPOëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‚®ì€ LR
        weight_decay=0.01,
        warmup_ratio=0.1,
        lr_scheduler_type="cosine",

        bf16=True,  # L4/A100ì€ BF16 native ì§€ì›
        gradient_checkpointing=True,  # Enable to save memory
        gradient_checkpointing_kwargs={"use_reentrant": False},  # Required for DPO
        logging_steps=10,
        eval_strategy="steps",
        eval_steps=50,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=3,

        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,

        report_to="none",  # MLflow ì—°ë™ ì‹œ ë³€ê²½
        max_length=512,  # Limit sequence length to save memory
        max_prompt_length=256,
    )

    # 4. DPO Trainer ì´ˆê¸°í™”
    print("Initializing DPO Trainer...")
    dpo_trainer = DPOTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset['train'],
        eval_dataset=dataset['test'],
        processing_class=tokenizer,  # ìµœì‹  ë²„ì „ì—ì„œëŠ” tokenizer ëŒ€ì‹  processing_class ì‚¬ìš©
    )

    # 5. í•™ìŠµ ì‹¤í–‰
    print("\nğŸš€ Starting DPO training...\n")
    dpo_trainer.train()

    # 6. ëª¨ë¸ ì €ì¥
    print(f"\nğŸ’¾ Saving model to {output_dir}...")
    dpo_trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)

    print(f"\nâœ… Training complete for {persona_id}!")

    return model


def main():
    parser = argparse.ArgumentParser(description="Train DPO model for a persona")
    parser.add_argument("--persona", type=str, required=True, help="Persona ID")
    parser.add_argument("--base_model", type=str, default="meta-llama/Llama-3.2-3B-Instruct")
    parser.add_argument("--adapter", type=str, default="../../models/llama3b_lambda_lora")
    parser.add_argument("--data_dir", type=str, default="../../data/dpo_training_data")
    parser.add_argument("--output_dir", type=str, default="../../models/dpo_personas")
    args = parser.parse_args()

    # í•™ìŠµ ë°ì´í„° íŒŒì¼
    train_file = f"{args.data_dir}/{args.persona}_dpo_train.jsonl"

    if not Path(train_file).exists():
        print(f"Error: Training file not found: {train_file}")
        return

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = f"{args.output_dir}/{args.persona}_v1.0"

    # DPO í•™ìŠµ ì‹¤í–‰
    train_dpo_persona(
        base_model_path=args.base_model,
        adapter_path=args.adapter,
        persona_id=args.persona,
        train_file=train_file,
        output_dir=output_dir
    )


if __name__ == "__main__":
    main()
