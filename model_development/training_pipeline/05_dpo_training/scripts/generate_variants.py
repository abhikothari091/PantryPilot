"""
í˜ë¥´ì†Œë‚˜ë³„ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± + 2ê°œ variants ìƒì„±
GPT-4ê°€ ë‚˜ì¤‘ì— chosen/rejected ê²°ì •
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import yaml
import json
from typing import Dict, List
from tqdm import tqdm
import random
from pathlib import Path
import argparse


class VariantGenerator:
    def __init__(self, base_model_path: str, adapter_path: str, personas_config: str):
        print(f"Loading model from {base_model_path} with adapter {adapter_path}...")

        # Device ì„¤ì • (MPS for M1/M2 Mac)
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
            print("ğŸš€ Using MPS (Metal Performance Shaders) for acceleration")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
            print("ğŸš€ Using CUDA GPU")
        else:
            self.device = torch.device("cpu")
            print("âš ï¸  Using CPU (slow)")

        # ëª¨ë¸ ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        self.tokenizer.pad_token = self.tokenizer.eos_token

        self.base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )
        self.model = PeftModel.from_pretrained(self.base_model, adapter_path)
        self.model = self.model.to(self.device)
        self.model.eval()

        print("âœ… Model loaded successfully")

        # í˜ë¥´ì†Œë‚˜ ë¡œë“œ
        with open(personas_config) as f:
            self.personas = yaml.safe_load(f)['personas']

        print(f"âœ… Loaded {len(self.personas)} personas")

    def create_user_message(self, inventory: List[str], persona: Dict) -> str:
        """
        í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ user message ìƒì„±
        """
        inventory_str = ", ".join(inventory)
        message = f"I have {inventory_str}."

        # í˜ë¥´ì†Œë‚˜ ì„ í˜¸ë„ ë°˜ì˜
        if persona.get('preferences', {}).get('cuisine'):
            cuisine = random.choice(persona['preferences']['cuisine'])
            message += f" I want a {cuisine} recipe."

        # ì‹ì´ ì œì•½ ë°˜ì˜
        if persona.get('dietary_restrictions'):
            restriction = random.choice(persona['dietary_restrictions'])
            phrasing = random.choice([
                f" I want a {restriction} recipe.",
                f" I'm {restriction}, what can I cook?",
                f" {restriction.capitalize()} recipe please."
            ])
            message += phrasing

        return message

    def create_chatml_prompt(self, user_message: str, persona: Dict,
                            enforce_constraints: bool = True) -> str:
        """
        ChatML í˜•ì‹ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            enforce_constraints: Trueë©´ í˜ë¥´ì†Œë‚˜ ì œì•½ ëª…ì‹œ, Falseë©´ ì¼ë°˜ì ì¸ í”„ë¡¬í”„íŠ¸
        """
        # System prompt
        system_prompt = "You are a recipe generation AI that creates recipes based on user inventory and preferences."

        if enforce_constraints:
            # í˜ë¥´ì†Œë‚˜ ì œì•½ ê°•í•˜ê²Œ ì ìš©

            # 1. Cuisine preference (ì¶”ê°€)
            if persona.get('preferences', {}).get('cuisine'):
                cuisines = ", ".join(persona['preferences']['cuisine'])
                system_prompt += f" You specialize in {cuisines} cuisine."

            # 2. Flavor profile (ì¶”ê°€)
            if persona.get('preferences', {}).get('flavor_profile'):
                flavors = ", ".join(persona['preferences']['flavor_profile'])
                system_prompt += f" You prefer {flavors} flavors."

            # 3. Preferred ingredients (ì¶”ê°€)
            if persona.get('preference_keywords'):
                keywords = ", ".join(persona['preference_keywords'][:5])
                system_prompt += f" Try to incorporate ingredients like: {keywords}."

            # 4. Dietary restrictions (ê¸°ì¡´)
            if persona.get('dietary_restrictions'):
                restrictions = ", ".join(persona['dietary_restrictions'])
                system_prompt += f" The user is {restrictions}."

            # 5. Forbidden ingredients (ê¸°ì¡´)
            if persona.get('forbidden_keywords'):
                forbidden = ", ".join(persona['forbidden_keywords'][:5])
                system_prompt += f" Do NOT use these ingredients: {forbidden}."

        # ChatML í˜•ì‹
        prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
{user_message}<|im_end|>
<|im_start|>assistant
"""

        return prompt

    def generate_response(self, prompt: str, temperature: float = 0.7) -> str:
        """
        ë ˆì‹œí”¼ ìƒì„± (JSON)
        """
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=temperature,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.convert_tokens_to_ids("<|im_end|>")
            )

        # Assistant ì‘ë‹µ ì¶”ì¶œ
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=False)

        if "<|im_start|>assistant" in full_response:
            assistant_response = full_response.split("<|im_start|>assistant")[-1]
            assistant_response = assistant_response.replace("<|im_end|>", "").strip()
            return assistant_response

        return full_response

    def generate_2_variants(self, user_message: str, persona: Dict) -> tuple:
        """
        2ê°œ variants ìƒì„±

        Variant A: í˜ë¥´ì†Œë‚˜ ì œì•½ ê°•í•˜ê²Œ ì ìš© (temperature=0.7)
        Variant B: í˜ë¥´ì†Œë‚˜ ì œì•½ ì•½í•˜ê²Œ ë˜ëŠ” ì—†ìŒ (temperature=0.9)

        Returns:
            (variant_a, variant_b, base_prompt)
        """
        # Variant A: ì œì•½ ê°•í•¨
        prompt_a = self.create_chatml_prompt(user_message, persona, enforce_constraints=True)
        variant_a = self.generate_response(prompt_a, temperature=0.7)

        # Variant B: ì œì•½ ì•½í•¨ (ë˜ëŠ” ë” ë‹¤ì–‘í•œ ì¶œë ¥)
        prompt_b = self.create_chatml_prompt(user_message, persona, enforce_constraints=False)
        variant_b = self.generate_response(prompt_b, temperature=0.9)

        # Base prompt (chosen/rejected ëª¨ë‘ì— ì‚¬ìš©ë  í”„ë¡¬í”„íŠ¸)
        # Variant Aì˜ prompt ì‚¬ìš© (ì œì•½ì´ ëª…ì‹œëœ ë²„ì „)
        base_prompt = prompt_a

        return variant_a, variant_b, base_prompt

    def generate_for_persona(self, persona_id: str, count: int = 500):
        """
        í˜ë¥´ì†Œë‚˜ë³„ë¡œ 500ê°œ Ã— 2 variants ìƒì„±
        """
        persona = self.personas[persona_id]
        samples = []

        print(f"\n{'='*60}")
        print(f"Generating {count} samples for: {persona_id}")
        print(f"Persona: {persona['name']}")
        print(f"{'='*60}\n")

        for i in tqdm(range(count), desc=f"{persona_id}"):
            # ì¬ë£Œ ì„ íƒ
            inventory = self._get_compatible_inventory(persona, count=random.randint(5, 8))

            # User message ìƒì„±
            user_message = self.create_user_message(inventory, persona)

            # 2ê°œ variants ìƒì„±
            try:
                variant_a, variant_b, base_prompt = self.generate_2_variants(user_message, persona)

                # ì €ì¥ (GPT-4ê°€ ë‚˜ì¤‘ì— chosen/rejected ê²°ì •)
                sample = {
                    "prompt": base_prompt,
                    "variant_a": variant_a,
                    "variant_b": variant_b,
                    "metadata": {
                        "persona": persona_id,
                        "user_message": user_message,
                        "inventory": inventory
                    }
                }

                samples.append(sample)
            except Exception as e:
                print(f"\nError generating sample {i}: {e}")
                continue

        return samples

    def _get_compatible_inventory(self, persona: Dict, count: int) -> List[str]:
        """
        í˜ë¥´ì†Œë‚˜ í˜¸í™˜ ì¬ë£Œ ì„ íƒ
        """
        all_ingredients = [
            "tofu", "chicken", "beef", "pork", "salmon", "shrimp", "eggs",
            "rice", "pasta", "bread", "quinoa", "couscous", "flour", "noodles",
            "onion", "garlic", "tomato", "bell pepper", "broccoli", "carrot",
            "spinach", "mushroom", "lettuce", "cucumber", "potato", "corn",
            "olive oil", "butter", "cheese", "milk", "yogurt",
            "soy sauce", "salt", "pepper", "ginger", "basil", "oregano",
            "beans", "lentils", "chickpeas", "avocado", "lime", "cilantro",
            "eggplant", "zucchini", "cabbage", "kale", "cauliflower",
            "green beans", "peas", "celery", "leek", "scallion"
        ]

        # Forbidden keywords ì œì™¸
        forbidden = [kw.lower() for kw in persona.get('forbidden_keywords', [])]
        compatible = [
            ing for ing in all_ingredients
            if not any(f in ing.lower() for f in forbidden)
        ]

        # ëœë¤ ì„ íƒ
        if len(compatible) < count:
            selected = compatible
        else:
            selected = random.sample(compatible, count)

        # ì„ í˜¸ ì¬ë£Œ ì¼ë¶€ í¬í•¨ (30% í™•ë¥ )
        preferred = persona.get('preference_keywords', [])
        if preferred and random.random() < 0.3:
            pref_candidates = [p for p in preferred if p in all_ingredients]
            if pref_candidates:
                pref_ing = random.choice(pref_candidates)
                if pref_ing not in selected and len(selected) > 0:
                    selected[0] = pref_ing

        return selected


def main():
    parser = argparse.ArgumentParser(description="Generate DPO variant pairs for personas")
    parser.add_argument("--base_model", default="meta-llama/Llama-3.2-3B-Instruct",
                       help="Base model path")
    parser.add_argument("--adapter", default="../04_training/llama3b_lambda_lora",
                       help="LoRA adapter path (relative to 04_training)")
    parser.add_argument("--personas_config", default="../05_dpo_training/personas.yaml",
                       help="Personas configuration file")
    parser.add_argument("--output_dir", default="../05_dpo_training/data/variants",
                       help="Output directory for variants")
    parser.add_argument("--count", type=int, default=500,
                       help="Number of samples per persona")
    parser.add_argument("--persona", type=str, default=None,
                       help="Generate for specific persona only (optional)")
    args = parser.parse_args()

    # ìƒì„±ê¸° ì´ˆê¸°í™”
    generator = VariantGenerator(
        args.base_model,
        args.adapter,
        args.personas_config
    )

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ìƒì„±í•  í˜ë¥´ì†Œë‚˜ ê²°ì •
    if args.persona:
        personas_to_generate = [args.persona] if args.persona in generator.personas else []
        if not personas_to_generate:
            print(f"Error: Persona '{args.persona}' not found in config")
            return
    else:
        personas_to_generate = list(generator.personas.keys())

    print(f"\nWill generate variants for {len(personas_to_generate)} persona(s)")
    print(f"Samples per persona: {args.count}")
    print(f"Total samples: {len(personas_to_generate) * args.count * 2} (2 variants each)\n")

    # ê° í˜ë¥´ì†Œë‚˜ë³„ ìƒì„±
    for persona_id in personas_to_generate:
        samples = generator.generate_for_persona(persona_id, args.count)

        # ì €ì¥
        output_file = output_dir / f"{persona_id}_variants.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        print(f"âœ… Saved {len(samples)} samples to {output_file}\n")

    print("\nğŸ‰ Variant generation complete!")


if __name__ == "__main__":
    main()
