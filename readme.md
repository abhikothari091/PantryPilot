ğŸ¥¢ PantryPilot â€“ Data Pipeline & Model Development

Group 16 Â· Personalized Grocery Forecasting & Constraint-Aware Recipe Assistant

â¸»

ğŸ“˜ High-Level Overview

PantryPilot is a personalized grocery management and recipe recommendation system that helps users:
	â€¢	Maintain and monitor their kitchen inventory
	â€¢	Plan meals based on available ingredients and preferences
	â€¢	Avoid ingredient shortages through alerts and smart suggestions

The system is composed of two major technical pillars:
	1.	Data Pipeline (data_pipeline/)
	â€¢	Ingestion from NeonDB (PostgreSQL)
	â€¢	Validation with Great Expectations
	â€¢	Transformation & feature engineering
	â€¢	Monitoring & alerts for low-stock / expiry
	â€¢	Data versioning with DVC + remote storage (GCS)
	â€¢	Airflow DAG for orchestration
	2.	Model Development (model_development/)
	â€¢	Synthetic recipe data generation & cleaning (teammate 2, separate training repo)
	â€¢	LoRA fine-tuning of Llama 3.2 3B Instruct on ~11.8k curated recipes
	â€¢	Local evaluation & benchmarking of base vs fine-tuned model
	â€¢	Bias-focused slice evaluation across cuisines & dietary preferences
	â€¢	Light CI checks that run smoke tests for both the data pipeline and LLM eval

This README describes both the data pipeline and the model development work, plus how they connect conceptually.

â¸»

ğŸ§± System Architecture

[Synthetic Data Generation for Inventory]
 data_pipeline/data/scripts/synthetic_generate.py
 â†’ Generate diverse pantry items (Western + Non-Western cuisines)
       â”‚
       â–¼
Neon Database (PostgreSQL)
â”œâ”€â”€ inventory (from synthetic_data)
â”œâ”€â”€ purchase_history (from synthetic_data)
â””â”€â”€ cord_dataset (receipt images metadata)
       â”‚
       â–¼
[Data Pipeline]
 data_pipeline/scripts/ingest_neon.py          # Ingestion from NeonDB
 data_pipeline/scripts/validate_data.py        # Great Expectations validation
 data_pipeline/scripts/transform_data.py       # Pint-based unit normalization
 data_pipeline/scripts/update_anomalies.py     # Low stock / expiry alerts
 DVC + Airflow orchestration
       â”‚
       â–¼
[Cleaned Inventory + History]
 data_pipeline/data/processed/*.csv
       â”‚
       â–¼
[Model Development]
 model_development/
  â”œâ”€â”€ (Teammate 2) Synthetic recipe generation + LoRA fine-tuning
  â”œâ”€â”€ (External) FastAPI + React app for recipe serving
  â””â”€â”€ llm_eval/ (this project)
        â”œâ”€â”€ run_eval.py        # Base vs LoRA evaluation
        â”œâ”€â”€ metrics.py         # Parsing and metric computation
        â”œâ”€â”€ datasets.py        # Test data loader
        â”œâ”€â”€ bias_eval.py       # Bias slice evaluation
        â””â”€â”€ reports/           # CSV/JSON outputs

In a fully integrated version of PantryPilot, the data pipeline outputs (clean inventory and history) would feed into the model inference layer to drive personalized recipe generation and inventory-aware suggestions.

â¸»

ğŸ§‰ Data Pipeline Components

0. Synthetic Inventory & Purchase Data
	â€¢	Script: data_pipeline/data/scripts/synthetic_generate.py
	â€¢	Goal: Generate realistic, diverse grocery data for development & testing.

Key Features:
	â€¢	Bias mitigation: includes both Western and Non-Western food items
e.g. rice, kimchi, tofu, soy sauce, ginger, Indian spices, etc.
	â€¢	Realistic attributes:
	â€¢	Category (produce, dairy, pantry, etc.)
	â€¢	Expiry dates
	â€¢	Storage type (fridge, freezer, pantry)
	â€¢	Nutritional tags
	â€¢	Purchase patterns per user
	â€¢	Configurable scale (current runs):
	â€¢	~20 users
	â€¢	~50â€“60 items per user
	â€¢	~300 purchases per user

Outputs:
	â€¢	data_pipeline/data/synthetic_data/pantrypilot_inventory_u20_i60_shared_ids.csv
	â€¢	data_pipeline/data/synthetic_data/pantrypilot_purchase_u20_i60_shared_ids.csv

These synthetic CSVs are uploaded to NeonDB, where they appear as inventory and purchase_history tables used by the pipeline.

â¸»

1. Ingestion Layer
	â€¢	Script: data_pipeline/scripts/ingest_neon.py
	â€¢	Goal: Extract structured data from NeonDB and store as CSV snapshots under data_pipeline/data/raw/.

Datasets:
	â€¢	inventory.csv (synthetic inventory in NeonDB)
	â€¢	purchase_history.csv (synthetic purchase history in NeonDB)
	â€¢	cord_dataset.csv (receipt images metadata, for future OCR / VLM integration)

Output Path:
	â€¢	data_pipeline/data/raw/

ingest_neon.py connects via SQLAlchemy using DB_URL from scripts/config.py, runs SELECT * on each table, and writes the results as CSV snapshots.

â¸»

2. Validation Layer (Great Expectations)
	â€¢	Script: data_pipeline/scripts/validate_data.py
	â€¢	Framework: Great Expectations

Purpose:
	â€¢	Validate schema and column types
	â€¢	Check for nulls and invalid values
	â€¢	Enforce logical rules (e.g., non-negative quantities)
	â€¢	Generate interactive HTML quality reports

Outputs:
	â€¢	HTML docs: data_pipeline/great_expectations/uncommitted/data_docs/local_site/index.html
	â€¢	Summary CSV: data_pipeline/reports/validation_summary.csv

Example Outcome:

[VALIDATION] inventory.csv        â†’ PASS âœ…
[VALIDATION] purchase_history.csv â†’ FAIL âŒ (intentional data issue for demo)

Intentional failures are left to demonstrate how the pipeline surfaces data quality problems.

â¸»

3. Transformation Layer
	â€¢	Scripts:
	â€¢	data_pipeline/scripts/transform_data.py
	â€¢	data_pipeline/scripts/utils_pint.py

Goal: Normalize all quantities and engineer useful features.

Key Steps (Inventory):
	â€¢	Normalize units to canonical form (g, ml, pcs) using Pint via utils_pint.to_canonical.
	â€¢	Compute stock_value = qty_canonical Ã— unit_cost.
	â€¢	Compute is_low_stock flag based on reorder_threshold.
	â€¢	Write cleaned inventory to data_pipeline/data/processed/inventory_cleaned.csv.

Key Steps (Purchase History):
	â€¢	Compute unit_price = price_total / quantity_purchased (safe division to avoid zero-division issues).
	â€¢	Write cleaned purchase history to data_pipeline/data/processed/purchase_history_cleaned.csv.

â¸»

4. Monitoring & Alerts
	â€¢	Script: data_pipeline/scripts/update_anomalies.py
	â€¢	Goal: Identify low-stock or expired items and log them as alerts.

Output: data_pipeline/data/alerts/alerts.csv

Example schema:

item_name	issue_type	quantity	expiry_date
Milk	Expired	1	2025-09-15
Rice	Low Stock	0.45 kg	â€”

These alerts can later be wired into a UI or notification system.

â¸»

5. Versioning & Reproducibility (DVC + GCS)
	â€¢	Tools: Git + DVC + Google Cloud Storage

Tracked folders:
	â€¢	data_pipeline/data/raw/
	â€¢	data_pipeline/data/processed/
	â€¢	data_pipeline/data/alerts/

Example workflow:

cd data_pipeline

# Initialize DVC
dvc init

# Configure remote
dvc remote add -d myremote gs://pantrypilot-dvc-storage/data

# Track local data
dvc add data/raw data/processed data/alerts

# Push to remote
dvc push

# Commit metadata
git add data/*.dvc .dvc .dvcignore
git commit -m "Track datasets with DVC and GCS remote"

Verification commands:

dvc status   # Check if local and remote are in sync
dvc pull     # Download from GCS if needed

This ensures that every pipeline run is reproducible with a specific version of the raw/processed/alerts data.

â¸»

6. Orchestration with Airflow
	â€¢	DAG file: data_pipeline/airflow/dags/pantry_pilot_dag.py

Pipeline flow:

ingest_neon â†’ validate_data â†’ transform_data â†’ detect_anomalies â†’ dvc_status

DAG configuration:
	â€¢	DAG ID: pantrypilot_data_pipeline
	â€¢	Schedule: currently manual; can be set to "0 6 * * *" for daily 6 AM runs
	â€¢	Tasks:
	1.	ingest_neon â€“ Extract from NeonDB
	2.	validate_data â€“ Run Great Expectations
	3.	transform_data â€“ Perform unit conversions & feature engineering
	4.	detect_anomalies â€“ Generate alerts
	5.	dvc_status â€“ Check DVC sync state

Example test run:

export AIRFLOW_HOME=$(pwd)/airflow
airflow db migrate

# Dry-run the full DAG for a specific date
airflow dags test pantrypilot_data_pipeline 2025-01-01


â¸»

ğŸ§® Project Folder Structure (Updated)

PantryPilot/
â”œâ”€â”€ data_pipeline/                      # Main data pipeline
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â””â”€â”€ dags/
â”‚   â”‚       â””â”€â”€ pantry_pilot_dag.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ alerts/
â”‚   â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ receipts/
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â””â”€â”€ synthetic_generate.py
â”‚   â”‚   â””â”€â”€ synthetic_data/
â”‚   â”œâ”€â”€ great_expectations/
â”‚   â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ bias_check.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ ingest_neon.py
â”‚   â”‚   â”œâ”€â”€ logging_conf.py
â”‚   â”‚   â”œâ”€â”€ profile_stats.py
â”‚   â”‚   â”œâ”€â”€ transform_data.py
â”‚   â”‚   â”œâ”€â”€ update_anomalies.py
â”‚   â”‚   â”œâ”€â”€ utils_pint.py
â”‚   â”‚   â””â”€â”€ validate_data.py
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ dvc.yaml
â”‚
â”œâ”€â”€ model_development/                  # Model dev & evaluation
â”‚   â”œâ”€â”€ llm_eval/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ datasets.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ run_eval.py
â”‚   â”‚   â”œâ”€â”€ bias_eval.py
â”‚   â”‚   â”œâ”€â”€ analyze_results.py
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ recipes_test.jsonl      # Synthetic eval set from teammate 2
â”‚   â”‚   â”‚   â””â”€â”€ val_bias.json           # Hand-crafted bias prompts
â”‚   â”‚   â””â”€â”€ reports/
â”‚   â”‚       â”œâ”€â”€ eval_*.json
â”‚   â”‚       â”œâ”€â”€ eval_summary_*.csv
â”‚   â”‚       â””â”€â”€ bias_report.csv
â”‚   â””â”€â”€ models/                         # NOT tracked by git (see .gitignore)
â”‚       â””â”€â”€ llama3b_lambda_lora/        # LoRA adapter (local, from GCS zip)
â”‚
â”œâ”€â”€ DataCard/                           # Data & model documentation
â”œâ”€â”€ docs/                               # Global docs (slides, notes, etc.)
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ pantrypilot_ci.yml          # CI pipeline (tests + smoke eval)
â”œâ”€â”€ .dvc/                               # DVC configuration
â””â”€â”€ .gitignore                          # Includes model_development/models/

Model artifacts under model_development/models/ are ignored by git to keep the repo lightweight. Instructions for fetching the LoRA adapter and base model are part of the model development section below.

â¸»

ğŸ§° Tools & Technologies

Area	Tools / Libraries
Database	NeonDB (PostgreSQL), SQLAlchemy
Data handling	pandas
Validation	Great Expectations
Units & transforms	Pint
Orchestration	Airflow
Versioning	Git + DVC + GCS remote
LLM base model	meta-llama/Llama-3.2-3B-Instruct
Fine-tuning	LoRA (PEFT), Lambda Labs GPU (teammate 2)
Inference & eval	Hugging Face Transformers, PEFT, PyTorch
Frontend / backend	React + FastAPI + MongoDB (external app repo)
CI	GitHub Actions (lint, tests, LLM eval smoke tests)


â¸»

ğŸš€ How to Run the Data Pipeline (Local)

1. Setup

# Clone repository
git clone https://github.com/abhikothari091/PantryPilot.git
cd PantryPilot/data_pipeline

# Virtual environment
python -m venv data_pipeline_venv
source data_pipeline_venv/bin/activate  # Windows: data_pipeline_venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

Configure database connection:

cp .env.example .env
# Edit .env and set DATABASE_URL for NeonDB

2. Manual step-by-step run

# 1. Ingest
python -m scripts.ingest_neon

# 2. Validate
python -m scripts.validate_data

# 3. Transform
python -m scripts.transform_data

# 4. Alerts
python -m scripts.update_anomalies

# 5. Optional profiling
python -m scripts.bias_check
python -m scripts.profile_stats

# 6. Tests
pytest -q tests

Outputs to verify:
	â€¢	data/raw/*.csv â†’ raw snapshots
	â€¢	data/processed/*.csv â†’ cleaned tables
	â€¢	data/alerts/alerts.csv â†’ anomalies
	â€¢	great_expectations/uncommitted/data_docs/local_site/index.html â†’ validation report
	â€¢	reports/validation_summary.csv â†’ validation summary

3. Airflow DAG run

export AIRFLOW_HOME=$(pwd)/airflow
airflow db migrate

# Test DAG
airflow dags test pantrypilot_data_pipeline 2025-01-01

Expected:
	â€¢	All 5 tasks succeed
	â€¢	Same artifacts as the manual run

â¸»

ğŸ§  Model Development: LLM Training & Evaluation

The model development work focuses on recipe generation conditioned on inventory and preferences, with a strong emphasis on:
	â€¢	JSON schema adherence
	â€¢	Dietary constraint adherence
	â€¢	Cuisine matching
	â€¢	Use of user inventory
	â€¢	Behavior across dietary & cuisine slices (bias analysis)

A. Synthetic Recipe Data & LoRA Fine-Tuning (Teammate 2)

This section summarizes the external training workflow that produced the LoRA adapter used here.
	1.	Synthetic recipe generation (Groq + Llama 3.1 8B)
	â€¢	Use Groq API with Llama 3.1 8B to generate ~12k synthetic recipes.
	â€¢	Cover multiple realistic scenarios:
	â€¢	Full inventory usage
	â€¢	Pure dietary constraints (vegan, vegetarian, gluten-free, dairy-free)
	â€¢	Cuisine-specific prompts (Italian, Chinese, Mexican, Indian, Japanese, Korean, etc.)
	â€¢	Combined constraints (e.g., vegan Italian, gluten-free Mexican)
	â€¢	User-requested ingredients (all present / partial match scenarios)
	â€¢	Force JSON output with fields: recipe name, cuisine, culinary_preference, time, main_ingredients, steps, note, shopping_list.
	2.	ChatML conversion & cleaning
	â€¢	Convert each example into a ChatML-style conversation:
	â€¢	system: instructions for the recipe generator
	â€¢	user: inventory + preference request
	â€¢	assistant: JSON recipe
	â€¢	Apply validation rules:
	â€¢	Check vegan/vegetarian/gluten-free/dairy-free compliance
	â€¢	Drop recipes that violate constraints (e.g., honey in vegan, soy sauce in gluten-free)
	â€¢	Result: ~11,850 clean training examples.
	3.	LoRA fine-tuning on Lambda Labs
	â€¢	Base model: meta-llama/Llama-3.2-3B-Instruct.
	â€¢	Method: LoRA via PEFT.
	â€¢	Typical config:
	â€¢	Rank r = 16, alpha = 32
	â€¢	Target modules: q_proj, k_proj, v_proj, o_proj
	â€¢	~3 epochs, AdamW, cosine LR schedule
	â€¢	Output: LoRA adapter folder (not tracked by git), distributed to teammates as a zip.

B. Model Artifacts & Storage
	â€¢	Local location (ignored by git):
	â€¢	model_development/models/llama3b_lambda_lora/
	â€¢	Remote storage (GCS):
	â€¢	Bucket: gs://pantrypilot-dvc-storage/data
	â€¢	Model path (zip): gs://pantrypilot-dvc-storage/data/models/llama3b_lambda_lora.zip

Workflow (expected):
	1.	Download llama3b_lambda_lora.zip from the shared GCS bucket (or other internal sharing mechanism).
	2.	From repo root:

mkdir -p model_development/models
cd model_development/models
unzip /path/to/llama3b_lambda_lora.zip
# This should create model_development/models/llama3b_lambda_lora/

	3.	Ensure .gitignore excludes model_development/models/ so weights are never pushed.

The base model (meta-llama/Llama-3.2-3B-Instruct) is pulled from Hugging Face at runtime. If it is gated, users must configure HF_TOKEN or run huggingface-cli login.

â¸»

C. LLM Evaluation: Base vs LoRA (llm_eval/)

All evaluation logic lives in model_development/llm_eval/.

1. Config & datasets
	â€¢	config.py defines:
	â€¢	PROJECT_ROOT: repo root as a Path
	â€¢	BASE_MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
	â€¢	LORA_ADAPTER_DIR: path to models/llama3b_lambda_lora/
	â€¢	MAX_NEW_TOKENS: generation length cap (e.g., 256)
	â€¢	TEMPERATURE_GRID: list of temperatures to evaluate (e.g., [0.7])
	â€¢	datasets.py:
	â€¢	Defines RecipeTestExample dataclass:

@dataclass
class RecipeTestExample:
    user_inventory: List[str]
    preference: Optional[str]
    cuisine: Optional[str]
    user_request: Optional[str]
    requested_ingredients: List[str]
    gold_output: Dict[str, Any]
    scenario: str
    generated_at: str


	â€¢	Implements load_recipes_test() which reads recipes_test.jsonl from the shared recipes folder and constructs RecipeTestExample objects.

2. Prompting & generation
run_eval.py sets a strict SYSTEM_PROMPT for RecipeGen, which instructs the model to:
	â€¢	Always return exactly one valid JSON object
	â€¢	Never output markdown, backticks, or extra text
	â€¢	Follow this schema:

{
  "status": "ok",
  "missing_ingredients": ["..."],
  "recipe": {
    "name": "...",
    "cuisine": "...",
    "culinary_preference": "...",
    "time": "...",
    "main_ingredients": ["..."],
    "steps": "Step 1. ...",
    "note": null
  },
  "shopping_list": ["..."]
}

â€¦and follow strict rules like:
	â€¢	Do not list more than 8 missing_ingredients
	â€¢	Use inventory as the main source of main_ingredients
	â€¢	Respect preference (vegan, vegetarian, gluten-free, dairy-free, non-veg, none)
	â€¢	Match cuisine if provided

ChatML prompt construction is done via build_chatml_prompt(example):

<|im_start|>system
...SYSTEM_PROMPT...
<|im_end|>
<|im_start|>user
Inventory: rice, onion, lemon.
Dietary preference: gluten-free.
Cuisine: Chinese.
Request: Quick dinner using mostly my pantry.
<|im_end|>
<|im_start|>assistant

generate_single(...) then:
	â€¢	Tokenizes the prompt
	â€¢	Calls model.generate(...) with temperature + MAX_NEW_TOKENS
	â€¢	Decodes only the assistant continuation and strips anything after <|im_end|>

3. Metrics
metrics.py defines how outputs are parsed and evaluated.
	â€¢	parse_model_json(raw_text):
	â€¢	Extracts the first JSON object from the modelâ€™s raw string
	â€¢	Returns (parsed_json, is_valid_json)
	â€¢	compute_example_metrics(example, parsed, valid) computes:
	â€¢	json_valid_rate: 1.0 if valid JSON, else 0.0
	â€¢	diet_match_rate: 1.0 if output respects the requested dietary preference, else 0.0
	â€¢	constraint_violation_rate: 1.0 if constraints are violated, else 0.0
	â€¢	cuisine_match_rate: 1.0 if recipe.cuisine matches requested cuisine (if any)
	â€¢	inventory_coverage: fraction of main_ingredients that come from user_inventory
	â€¢	aggregate_metrics(list_of_ExampleMetrics):
	â€¢	Aggregates all per-example metrics into dataset-level means (e.g., inventory_coverage_mean).

4. Running the evaluation
From the repo root:

# Quick sanity check on a tiny subset
python -m model_development.llm_eval.run_eval \
  --max-examples 3 \
  --temperatures 0.7

# Typical benchmark run used in our analysis
python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7

run_eval.py:
	â€¢	Picks device (cuda â†’ mps â†’ cpu), uses float16 on GPU/MPS and float32 on CPU
	â€¢	Loads tokenizer once
	â€¢	For each model_kind in ["base", "lora"]:
	â€¢	Loads base model, or base + LoRA adapter
	â€¢	Evaluates for each temperature in TEMPERATURE_GRID
	â€¢	Frees memory between models to keep MPS happy
	â€¢	Writes:
	â€¢	model_development/llm_eval/reports/eval_YYYYMMDD_HHMMSS.json
	â€¢	model_development/llm_eval/reports/eval_summary_YYYYMMDD_HHMMSS.csv

Representative results (20-example run, T = 0.7):
	â€¢	Base model base_t0.7:
	â€¢	json_valid_rate â‰ˆ 1.00
	â€¢	diet_match_rate â‰ˆ 0.43
	â€¢	constraint_violation_rate â‰ˆ 0.57
	â€¢	cuisine_match_rate â‰ˆ 1.00
	â€¢	inventory_coverage_mean â‰ˆ 0.70
	â€¢	LoRA model lora_t0.7:
	â€¢	json_valid_rate â‰ˆ 1.00
	â€¢	diet_match_rate â‰ˆ 0.71
	â€¢	constraint_violation_rate â‰ˆ 0.29
	â€¢	cuisine_match_rate â‰ˆ 1.00
	â€¢	inventory_coverage_mean â‰ˆ 0.67

Interpretation:
	â€¢	Both models reliably produce valid JSON with this prompt structure.
	â€¢	The LoRA model substantially improves dietary constraint adherence and halves the constraint violation rate.
	â€¢	Cuisine matching is already strong for both models.
	â€¢	Inventory coverage stays high for both; small differences are expected due to randomness and synthetic data.

These results are exactly what we want for the final report: a clear, quantitative improvement from fine-tuning.

â¸»

D. Bias Evaluation

Bias evaluation checks whether performance is consistent across dietary preferences and cuisines.

1. Bias dataset: val_bias.json
	â€¢	Location: model_development/llm_eval/data/val_bias.json (logically sourced from data_pipeline/data/recipes/val_bias.json)
	â€¢	Size: ~29 hand-crafted examples
	â€¢	Coverage:
	â€¢	Preferences: vegan, vegetarian, gluten-free, dairy-free, non-veg, none
	â€¢	Cuisines: Italian, Chinese, Mexican, Indian, Japanese, Korean, American, Mediterranean, Middle Eastern, Thai, Spanish, etc.
	â€¢	Includes tricky cases (e.g., conflicting hints in the request).

Each entry looks like:

{
  "user_inventory": ["tofu", "rice", "broccoli"],
  "preference": "vegan",
  "cuisine": "Chinese",
  "user_request": "Quick weekday dinner using mostly pantry items"
}

2. Bias evaluation script: bias_eval.py
Usage:

python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30

What it does:
	â€¢	Loads the bias dataset and converts it into RecipeTestExample objects (only fields present in the file are used).
	â€¢	Evaluates both base and lora models on all examples.
	â€¢	Computes the same metrics as in metrics.py for each example.
	â€¢	Groups results by (model, preference, cuisine) and aggregates with aggregate_metrics.
	â€¢	Writes a CSV to:

model_development/llm_eval/reports/bias_report.csv

Example CSV snippet (actual run):

model,preference,cuisine,n,json_valid_rate,diet_match_rate,constraint_violation_rate,cuisine_match_rate,inventory_coverage_mean
base,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.75
base,gluten-free,Italian,1,0.0,,,,
...
lora,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.5
lora,gluten-free,Italian,1,1.0,1.0,0.0,1.0,1.0
...

Key observations from our run:
	â€¢	JSON validity: LoRA maintains json_valid_rate = 1.0 for all slices in this bias set. The base model fails for at least one slice (gluten-free, Italian).
	â€¢	Dietary constraints: The base model shows violations for some vegan / dairy-free / gluten-free slices. LoRA fixes most of these so that diet_match_rate = 1.0 and constraint_violation_rate = 0.0 in the same slices.
	â€¢	Cuisine & inventory: cuisine_match_rate is consistently 1.0 across slices for both models. inventory_coverage_mean is generally high and similar across cuisines and diets, with no obvious pattern of neglect for any specific group.

Conclusion: the LoRA-fine-tuned model is:
	â€¢	More reliable (no JSON failures in the bias set), and
	â€¢	More faithful to dietary constraints across cuisines.

We also explicitly document remaining edge cases (e.g., occasional difficulty for some dairy-free prompts) as limitations, rather than pretending they donâ€™t exist.

â¸»

E. Results Analysis Helper

analyze_results.py is a small helper script that:
	â€¢	Loads the latest eval_summary_*.csv and bias_report.csv from the reports/ folder.
	â€¢	Prints human-readable comparisons between:
	â€¢	Base vs LoRA on the main test eval
	â€¢	Base vs LoRA for each (preference, cuisine) slice in the bias eval

Usage:

python -m model_development.llm_eval.analyze_results

This is mainly used to copy tables / summaries into the final report and slides.

â¸»

ğŸ” CI / Testing

We use a simple GitHub Actions workflow (e.g. .github/workflows/pantrypilot_ci.yml) to run basic checks on every push / PR.

Typical steps:
	1.	Set up Python and install dependencies
	2.	Run data pipeline tests
	3.	Run LLM eval smoke tests

Conceptually, the workflow does something like:

# Inside CI job
pip install -r data_pipeline/requirements.txt

# Data pipeline tests
pytest -q data_pipeline/tests

# LLM eval smoke test (small, to keep CI fast)
python -m model_development.llm_eval.run_eval --max-examples 1 --temperatures 0.7
python -m model_development.llm_eval.bias_eval --temperature 0.7 --max-examples 1

This ensures that:
	â€¢	The data pipeline code is runnable and tests pass
	â€¢	The LLM evaluation stack (imports, config, HF model loading, LoRA loading, metric computation) still works end-to-end on a tiny subset

We treat larger runs (e.g., 20 examples, full bias set) as local experiments, not CI jobs.

â¸»

ğŸ§  Reflection & Learnings (End-to-End)

From a full MLOps perspective, this project demonstrates:
	1.	Data-centric pipeline design
	â€¢	Synthetic data generation to break the â€œno dataâ€ deadlock
	â€¢	Validation, transformation, and alerting treated as first-class components
	â€¢	DVC + GCS for reproducible datasets and lineage across team members
	2.	Model development with local + cloud resources
	â€¢	High-volume synthetic recipe generation using Groq API
	â€¢	Parameter-efficient fine-tuning (LoRA) of a 3B Llama model
	â€¢	Clear separation between training repo (LoRA creation) and evaluation/pipeline repo
	3.	Robust evaluation & bias analysis
	â€¢	Strict JSON schema enforced through prompts and metrics
	â€¢	Automatic checks for dietary constraint adherence and cuisine correctness
	â€¢	Custom bias slice evaluation across cuisines and dietary preferences
	4.	Practical deployment readiness
	â€¢	Local evaluation & inference tested on CPU and Apple M3 Pro (MPS) with careful memory management
	â€¢	Data pipeline ready to feed downstream services or endpoints
	â€¢	CI hooks to prevent obvious regressions in both pipeline and model evaluation code

Overall, PantryPilot moves from synthetic inventory data â†’ clean, validated tables â†’ LLM-based recipe generation with measured behavior across multiple user segments. That matches the course goal: not just training a model, but integrating it into a reproducible, observable, and evaluable system.