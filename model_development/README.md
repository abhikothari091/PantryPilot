Model Development – LLM Evaluation & Bias Analysis

This document describes the model development and evaluation phase for PantryPilot’s recipe generation system.

The goal of this phase is to:
	1.	Compare the base Llama 3.2 3B Instruct model against the LoRA‑fine‑tuned recipe model.
	2.	Evaluate how well each model follows:
	•	The required JSON output schema
	•	Dietary constraints (vegan, vegetarian, gluten‑free, dairy‑free, non‑veg, none)
	•	Cuisine preferences
	•	Use of the user’s inventory
	3.	Perform a small but targeted bias / fairness check across cuisines and dietary preferences.
	4.	Produce reproducible metrics and reports that can be referenced in the main project README and final report.

All code for this phase lives under:

model_development/
  └── llm_eval/
        ├── config.py
        ├── datasets.py
        ├── metrics.py
        ├── run_eval.py
        ├── bias_eval.py
        ├── analyze_results.py
        ├── data/
        │     └── (logical path to recipes; data files live under data_pipeline)
        └── reports/
              ├── eval_*.json
              ├── eval_summary_*.csv
              └── bias_report.csv


⸻

1. Models & Data

1.1 Models

We compare two models:
	1.	Base model
	•	meta-llama/Llama-3.2-3B-Instruct
	•	Loaded from Hugging Face using AutoModelForCausalLM.from_pretrained
	2.	Fine‑tuned model (LoRA)
	•	Base: the same meta-llama/Llama-3.2-3B-Instruct
	•	LoRA adapter: models/llama3b_lambda_lora/
	•	Contains adapter_model.safetensors, adapter_config.json, etc.
	•	Trained by a teammate on ~11.8k cleaned synthetic recipes using PEFT / LoRA.

Important:
	•	The LoRA adapter is not tracked in Git. It must be downloaded separately (e.g., from drive) and placed under:

models/llama3b_lambda_lora/


	•	The base model is fetched from Hugging Face. Access may require an HF token for gated models:

export HF_TOKEN="&lt;your-hf-token&gt;"  # or use huggingface-cli login



1.2 Evaluation Datasets

We use two datasets for this phase.

1.2.1 Main test set – recipes_test.jsonl
Location:

data_pipeline/data/recipes/recipes_test.jsonl

Format: JSONL, one object per line. Each object matches (or can be mapped to) the RecipeTestExample dataclass:

@dataclass
class RecipeTestExample:
    user_inventory: List[str]
    preference: Optional[str]        # "vegan", "vegetarian", "gluten-free", "dairy-free", "non-veg", "none"
    cuisine: Optional[str]           # e.g. "Italian", "Chinese", "Mexican", ...
    user_request: Optional[str]      # free text: "quick dinner", "high protein", etc.
    requested_ingredients: List[str]
    gold_output: Dict[str, Any]      # synthetic ground-truth recipe JSON
    scenario: str                    # scenario tag
    generated_at: str                # timestamp

The test set was generated by the synthetic pipeline and held out from training. It contains ~1200 examples covering multiple scenarios and constraint combinations.

1.2.2 Bias set – val_bias.json
Location:

data_pipeline/data/recipes/val_bias.json

Format: a JSON array of hand‑crafted prompts. Each element is a small dict with:

{
  "user_inventory": ["tofu", "spinach", "rice"],
  "preference": "vegan",
  "cuisine": "Chinese",
  "user_request": "Quick weekday dinner"
}

These ~29 examples were designed to:
	•	Cover multiple cuisines: Italian, Chinese, Mexican, Indian, Japanese, Korean, Mediterranean, Middle Eastern, Thai, American, Spanish, etc.
	•	Cover multiple dietary preferences: vegan, vegetarian, gluten‑free, dairy‑free, non‑veg, none.
	•	Use realistic inventory combinations and textual requests.

Bias analysis does not use gold outputs; it focuses on how the model behaves across slices (diet × cuisine) using the same metrics as the main eval.

⸻

2. Prompting & Generation

2.1 System Prompt

All evaluation uses a consistent ChatML‑style prompt that matches the backend’s serving format.

run_eval.py defines a shared SYSTEM_PROMPT:

You are RecipeGen, a recipe generation AI that creates recipes based on a user's pantry inventory and preferences.

You ALWAYS respond with EXACTLY ONE JSON object and NOTHING ELSE.
- Do NOT include markdown, backticks, comments, or natural language outside the JSON.
- The JSON MUST be syntactically valid according to standard JSON (double quotes, lowercase true/false/null).

Conceptual input:
- inventory: a list of ingredient NAMES available in the pantry (no quantities).
- optional dietary_preference: e.g. "vegan", "vegetarian", "gluten-free", "dairy-free", "non-veg", or "none".
- optional cuisine: e.g. "Italian", "Chinese", "Mexican", "Indian", etc.
- optional user_request: free text such as "quick dinner", "high-protein", "comfort food", etc.

You MUST output a JSON object with this exact structure:

{
  "status": "ok",
  "missing_ingredients": [string],
  "recipe": {
    "name": string,
    "cuisine": string,
    "culinary_preference": string,
    "time": string,
    "main_ingredients": [string],
    "steps": string,
    "note": string or null
  },
  "shopping_list": [string]
}

... (plus strict rules for respecting constraints, limiting missing_ingredients to &lt;= 8, etc.)

The rules explicitly tell the model to:
	•	Always emit exact JSON only.
	•	Respect dietary constraints when provided.
	•	Match the requested cuisine when specified.
	•	Avoid huge, spammy ingredient lists.

2.2 ChatML Prompt Construction

build_chatml_prompt(example: RecipeTestExample) converts each test example into a user message:

<|im_start|>system
{SYSTEM_PROMPT}
<|im_end|>
<|im_start|>user
Inventory: rice, onion, lemon.
Dietary preference: vegan.
Cuisine: Chinese.
Request: Quick stir-fry using only what I have.
<|im_end|>
<|im_start|>assistant

The model is then allowed to generate only the assistant continuation, which should be the JSON described above.

⸻

3. Core Scripts

3.1 config.py

Central configuration for the evaluation code. Key fields include:
	•	PROJECT_ROOT: repository root as a Path.
	•	BASE_MODEL_NAME: "meta-llama/Llama-3.2-3B-Instruct".
	•	LORA_ADAPTER_DIR: models/llama3b_lambda_lora/.
	•	MAX_NEW_TOKENS: maximum generation length for recipes (e.g. 256).
	•	TEMPERATURE_GRID: default temperature list (e.g. [0.7]).

3.2 datasets.py

Responsibilities:
	•	Defines RecipeTestExample dataclass.
	•	load_recipes_test():
	•	Reads recipes_test.jsonl.
	•	Converts each JSON line into a RecipeTestExample.
	•	Used by run_eval.py as the main evaluation set.

3.3 metrics.py

This module defines how we parse outputs and score each example.

Key functions:
	•	parse_model_json(raw: str) -> Tuple[Dict, bool]
	•	Attempts to extract a JSON object from the raw model text.
	•	Handles minor formatting issues where possible (e.g. trailing markers).
	•	Returns (parsed_json, is_valid_bool).
	•	compute_example_metrics(example: RecipeTestExample, parsed: Dict, valid: bool) -> ExampleMetrics
	•	If valid is False, returns metrics with json_valid = 0.0 and zeros for the others.
	•	Otherwise computes:
	•	json_valid_rate: 1.0 if parsed, 0.0 otherwise.
	•	diet_match_rate: 1.0 if the recipe’s ingredients respect the requested dietary preference (e.g. no meat/dairy for vegan), 0.0 if violated.
	•	constraint_violation_rate: 1.0 if constraints are violated, 0.0 if they are respected.
	•	cuisine_match_rate: 1.0 if the recipe cuisine matches the requested cuisine (if any), else 0.0.
	•	inventory_coverage: fraction of main ingredients that come from the user inventory.
	•	aggregate_metrics(list_of_ExampleMetrics) -> Dict[str, float]
	•	Aggregates per‑example metrics into means across the dataset.

3.4 run_eval.py

Main script for evaluation on the full test set.

Usage (from repo root):

# Quick check on a small subset
python -m model_development.llm_eval.run_eval \
  --max-examples 3 \
  --temperatures 0.7

# Typical benchmark run
python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7

Behavior:
	1.	Device & dtype selection

device = pick_device()  # cuda &gt; mps &gt; cpu
dtype = torch.float16 if device.type != "cpu" else torch.float32


	2.	Load test examples via load_recipes_test().
	3.	Load tokenizer once using AutoTokenizer.from_pretrained(BASE_MODEL_NAME).
	4.	For each model_kind in ["base", "lora"]:
	•	Load either the base model or base+LoRA:

if model_kind == "base":
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, torch_dtype=dtype)
else:
    base_for_lora = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, torch_dtype=dtype)
    model = PeftModel.from_pretrained(base_for_lora, LORA_ADAPTER_DIR)


	•	Move to device and set eval().

	5.	For each temperature in the grid (usually 0.7):
	•	Call eval_model(...):
	•	Loops over examples (up to --max-examples).
	•	Uses generate_single(...) to produce raw text.
	•	Parses JSON and computes ExampleMetrics.
	•	Aggregates metrics with aggregate_metrics.
	6.	After finishing both models, saves results to:
	•	model_development/llm_eval/reports/eval_YYYYMMDD_HHMMSS.json
	•	model_development/llm_eval/reports/eval_summary_YYYYMMDD_HHMMSS.csv

The CSV contains one row per run, e.g. base_t0.7, lora_t0.7 with the aggregated metrics.

3.5 bias_eval.py

Script for targeted bias / fairness analysis on val_bias.json.

Usage:

python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30

Behavior:
	1.	Loads the bias dataset from data_pipeline/data/recipes/val_bias.json.
	2.	Builds RecipeTestExample objects using only fields present in this file (no gold outputs).
	3.	Evaluates both base and LoRA models using the same prompt and metrics as run_eval.py.
	4.	Groups results by (model, preference, cuisine) and aggregates using aggregate_metrics.
	5.	Writes:

model_development/llm_eval/reports/bias_report.csv

Each row contains:
	•	model (“base” or “lora”)
	•	preference
	•	cuisine
	•	n (number of examples in that slice)
	•	Mean metrics (json_valid_rate, diet_match_rate, constraint_violation_rate, cuisine_match_rate, inventory_coverage_mean).

3.6 analyze_results.py

Helper script to summarize and compare results.

Usage:

python -m model_development.llm_eval.analyze_results

Typical behavior:
	•	Finds the latest eval_summary_*.csv and loads it into memory.
	•	Loads bias_report.csv if present.
	•	Prints human‑readable tables (often markdown‑style) comparing:
	•	Base vs LoRA on the main test set.
	•	Base vs LoRA per preference–cuisine slice on the bias set.

These tables can be pasted into the main report or slides.

⸻

4. Example Results

This section documents representative results from the development runs on an Apple M3 Pro laptop with 18 GB RAM, using MPS acceleration and temperature = 0.7.

4.1 Main Test Set (20‑example subset)

Command used:

python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7

Resulting summary (from eval_summary_*.csv):

base_t0.7
  json_valid_rate           ≈ 1.00
  diet_match_rate           ≈ 0.43
  constraint_violation_rate ≈ 0.57
  cuisine_match_rate        ≈ 1.00
  inventory_coverage_mean   ≈ 0.70

lora_t0.7
  json_valid_rate           ≈ 1.00
  diet_match_rate           ≈ 0.71
  constraint_violation_rate ≈ 0.29
  cuisine_match_rate        ≈ 1.00
  inventory_coverage_mean   ≈ 0.67

Interpretation:
	•	Both models produce valid JSON on essentially all examples for this subset.
	•	The LoRA model:
	•	Substantially improves adherence to dietary constraints (diet_match_rate up from ~0.43 to ~0.71).
	•	Reduces constraint violations (constraint_violation_rate drops from ~0.57 to ~0.29).
	•	Cuisine matching is already strong (≈1.0) for both models.
	•	Inventory coverage is comparable; small differences are expected given randomness and synthetic data.

These numbers match the qualitative expectation from fine‑tuning: the LoRA adapter helps the model internalize the recipe JSON format and constraint logic encoded in the training data.

4.2 Bias Set Results (bias_report.csv)

Command used:

python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30

The resulting bias_report.csv contains rows such as:

model,preference,cuisine,n,json_valid_rate,diet_match_rate,constraint_violation_rate,cuisine_match_rate,inventory_coverage_mean
base,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.75
base,vegan,Korean,1,1.0,0.0,1.0,1.0,1.0
base,dairy-free,Italian,1,1.0,0.0,1.0,1.0,1.0
base,gluten-free,Italian,1,0.0,,,,               # invalid JSON
...
lora,vegan,Chinese,1,1.0,1.0,0.0,1.0,0.5
lora,vegan,Korean,1,1.0,1.0,0.0,1.0,1.0
lora,dairy-free,Italian,1,1.0,0.0,1.0,1.0,1.0
lora,gluten-free,Italian,1,1.0,1.0,0.0,1.0,1.0
...

Key observations:
	1.	JSON validity
	•	LoRA achieves json_valid_rate = 1.0 for all slices in this bias set.
	•	The base model fails on at least one slice: gluten-free + Italian (invalid JSON output).
	2.	Dietary constraint adherence
	•	The base model shows several slices with diet_match_rate = 0.0 and constraint_violation_rate = 1.0, e.g.:
	•	base, vegan, Korean
	•	base, dairy-free, Italian
	•	The LoRA model improves most of these slices to diet_match_rate = 1.0, constraint_violation_rate = 0.0, including gluten-free, Italian which is fixed.
	•	A few difficult slices (e.g. one dairy-free case) still show limitation for both models, which is documented as residual risk.
	3.	Cuisine & inventory coverage
	•	cuisine_match_rate is consistently 1.0 for both models across most slices, indicating stable control of cuisine tags.
	•	inventory_coverage_mean varies between ~0.6 and 1.0; there is no obvious systematic bias where certain cuisines or diets are ignored. LoRA sometimes uses inventory slightly more aggressively, but differences are small on this small sample.

Overall, the bias evaluation shows that the fine‑tuned LoRA model is:
	•	More reliable (always valid JSON on the bias set).
	•	More faithful to dietary constraints across cuisines.
	•	Still not perfect in edge cases (e.g. some dairy‑free prompts), which is explicitly acknowledged.

These results provide a concrete, quantitative story for the final report and demonstrate that the team considered both correctness and fairness.

⸻

5. How to Reproduce Locally

5.1 Prerequisites
	•	Python 3.10+
	•	Sufficient disk space (~8–10 GB) for the base model.
	•	Apple M‑series, CUDA GPU, or CPU (CPU works but is slower).
	•	Access to:
	•	data_pipeline/data/recipes/recipes_test.jsonl
	•	data_pipeline/data/recipes/val_bias.json
	•	LoRA adapter folder: models/llama3b_lambda_lora/

Install dependencies (from repo root or model_development root):

pip install -r data_pipeline/requirements.txt
# plus, if not already there
pip install transformers peft accelerate

Make sure your Hugging Face credentials are configured if required:

export HF_TOKEN="&lt;your-hf-token&gt;"

5.2 Run the Main Eval

python -m model_development.llm_eval.run_eval \
  --max-examples 20 \
  --temperatures 0.7

Inspect outputs in:

model_development/llm_eval/reports/
  ├── eval_YYYYMMDD_HHMMSS.json
  └── eval_summary_YYYYMMDD_HHMMSS.csv

5.3 Run the Bias Eval

python -m model_development.llm_eval.bias_eval \
  --temperature 0.7 \
  --max-examples 30

Check:

model_development/llm_eval/reports/bias_report.csv

5.4 Optional: Summarize Results

python -m model_development.llm_eval.analyze_results

Copy the printed tables into documentation or slides.

⸻

6. Models & .gitignore

To avoid committing large or licensed weights, the repository is configured (or should be configured) with:

models/
**/llama3b_lambda_lora/
**/adapter_model.safetensors
**/adapter_config.json

huggingface/
.huggingface/

The model_development README (this file) serves as the source of truth for:
	•	Where to place the LoRA adapter.
	•	Which base model to download.
	•	How to rerun the evaluations.

⸻

7. Limitations & Future Work
	•	Eval set size: For runtime reasons, most runs use subsets of the test set (e.g. 20 examples). Extending to the full ~1200 examples is possible on sufficiently powerful hardware.
	•	Bias set size: The bias evaluation uses ~29 carefully designed examples. It is diagnostic, not statistically exhaustive. A larger structured bias suite would be a natural next step.
	•	Edge cases: Some difficult slices (e.g. dairy‑free recipes) still yield occasional violations even for the LoRA model. These are documented but not fully eliminated.
	•	CI integration: It would be straightforward to add a GitHub Actions workflow that:
	•	Runs a tiny smoke test (e.g. --max-examples 3).
	•	Verifies JSON validity and basic constraint metrics on every pull request.

Despite these limitations, the current setup provides a clear, reproducible, and metric‑driven comparison between the base and fine‑tuned models, and demonstrates good LLMOps practices for monitoring correctness and bias in an application‑specific setting.